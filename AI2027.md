This is an audio version of AI 2027 by Daniel Coccatello, Scott Alexander, Thomas Larson, Eli Lifland, and Romeo Dean. We predict that the impact of superhuman AI over the next decade will be enormous, exceeding that of the Industrial Revolution. We wrote a scenario that represents our best guess about what that might look like.
こちらは、ダニエル・コカテロ、スコット・アレクサンダー、トーマス・ラーソン、イーライ・リフランド、ロミオ・ディーンによる『AI 2027』の音声版です。私たちは、今後10年間にわたる超人的なAIの影響が、産業革命をも上回るほど甚大なものになると予測しています。これは、その未来がどのようなものになるかについて、私たちなりの最善の予想をもとに描いたシナリオです。

It's informed by trend extrapolations, war games, expert feedback, experience at OpenAI, and previous forecasting successes.
これは、トレンドの外挿、ウォーゲーム、専門家のフィードバック、OpenAIでの経験、そして過去の予測成功事例に基づいています。

Heading, mid-2025, stumbling agents.
見出し、2025年半ば、つまずくエージェント。

The world sees its first glimpse of AI agents. Advertisements for computer-using agents emphasize the term personal assistant. You can prompt them with tasks like, order me a burrito on DoorDash, or open my budget spreadsheet and sum this month's expenses.
世界はAIエージェントの最初の片鱗を目にします。コンピュータを利用するエージェントの広告では、「パーソナルアシスタント」という言葉が強調されています。例えば、「DoorDashでブリトーを注文して」とか、「予算のスプレッドシートを開いて今月の経費を合計して」といったタスクを指示できます。

They will check in with you as needed, for example, to ask you to confirm purchases. At first, most people are reluctant to allow purchases without oversight. Over the next few years, automatically allowing small purchases becomes normalized as the AIs become more reliable and build up trust.
必要に応じて、例えば購入を確認するために、彼らはあなたに確認を取ります。当初、ほとんどの人は監視なしでの購入を許可することに消極的です。その後数年で、AIの信頼性が向上し信頼を構築するにつれて、少額の購入を自動的に許可することが常態化します。

Though more advanced than previous iterations, like Operator, they struggle to get widespread usage.
以前のイテレーション（例えばOperatorのようなもの）よりも高度であるものの、広範な利用を得るのに苦労しています。

Meanwhile, out of public focus, more specialized coding and research agents are beginning to transform their professions. The AIs of 2024 could follow specific instructions. They could turn bullet points into emails, and simple requests into working code.
一方、一般の注目が集まらないところで、より専門化されたコーディングおよびリサーチエージェントが、それぞれの専門職を変革し始めています。2024年のAIは特定の指示に従うことができました。箇条書きをメールに変換したり、簡単なリクエストを実行可能なコードに変換したりすることができました。

In 2025, AIs function more like employees. Coding AIs increasingly look like autonomous agents rather than mere assistants, taking instructions via Slack or Teams and making substantial code changes on their own, sometimes saving hours or even days. Research agents spend half an hour scouring the internet to answer your question.
2025年には、AIはより従業員のように機能します。コーディングAIは、単なるアシスタントではなく自律的なエージェントのように見え始めており、SlackやTeams経由で指示を受け、独自に大幅なコード変更を行い、時には数時間、あるいは数日を節約します。リサーチエージェントは、あなたの質問に答えるために30分かけてインターネットを徹底的に調査します。

The agents are impressive in theory and in cherry-picked examples, but in practice unreliable. AI Twitter is full of stories about tasks bungled in some particularly hilarious way. The better agents are also expensive.
これらのエージェントは理論上や選りすぐりの例では印象的ですが、実際には信頼性がありません。AI関連のTwitterは、特に滑稽な方法で失敗したタスクの話で溢れています。より優れたエージェントは高価でもあります。

You get what you pay for, and the best performance costs hundreds of dollars a month. Still, many companies find ways to fit AI agents into their workflows.
支払った分だけのものは得られ、最高のパフォーマンスには月額数百ドルかかります。それでも、多くの企業がAIエージェントをワークフローに組み込む方法を見つけています。

Heading. Late 2025. The world's most expensive AI.
見出し。2025年後半。世界で最も高価なAI。

OpenBrain is building the biggest data centers the world has ever seen. To avoid singling out any one existing company, we're going to describe a fictional artificial general intelligence company, which we'll call OpenBrain. We imagine the others to be three to nine months behind OpenBrain.
OpenBrainは、世界がかつて見たことのない最大のデータセンターを建設しています。既存の特定の企業を名指しするのを避けるため、ここではOpenBrainと呼ぶ架空の汎用人工知能企業について説明します。他の企業はOpenBrainより3ヶ月から9ヶ月遅れていると想像しています。

GPT-4 required 2 times 10 to the power of 25 flop of compute to train. Compute is measured in floating point operations, or flop, how many individual addition and multiplication operations occur over the course of running a program. OpenBrain's latest public model, Agent Zero, was trained with 10 to the power of 27 flop.
GPT-4のトレーニングには、2×10の25乗フロップの計算能力が必要でした。計算能力は浮動小数点演算（flop）で測定され、これはプログラム実行中に個々の加算および乗算演算が何回発生するかを示します。OpenBrainの最新の公開モデルであるAgent Zeroは、10の27乗フロップでトレーニングされました。

Once the new data centers are up and running, they'll be able to train a model with 10 to the power of 28 flop, a thousand times more than GPT-4. They could train this model given 150 days. See our computer supplement for more details.
新しいデータセンターが稼働すれば、GPT-4の1000倍にあたる10の28乗フロップでモデルをトレーニングできるようになります。このモデルは150日あればトレーニング可能です。詳細については、当社のコンピューター補足資料をご覧ください。

Other companies pour money into their own giant data centers, hoping to keep pace. Although models are improving on a wide range of skills, one stands out. OpenBrain focuses on AIs that can speed up AI research.
他の企業も、追いつこうと自社の巨大データセンターに資金を注ぎ込んでいます。モデルは広範なスキルで向上していますが、1つ際立っているものがあります。OpenBrainは、AI研究を加速できるAIに焦点を当てています。

They want to win the twin arms races against China, whose leading company we'll call DeepScent, and their US competitors. The more of their research and development, or R&D, cycle they can automate, the faster they can go.
彼らは、中国（その主要企業をDeepScentと呼びます）および米国の競合他社との二つの軍拡競争に勝ちたいと考えています。研究開発（R&D）サイクルを自動化できればできるほど、より速く進むことができます。

So when OpenBrain finishes training Agent One, a new model under internal development, it's good at many things, but great at helping with AI research. By this point, finishes training is a bit of a misnomer.
そのため、OpenBrainが社内で開発中の新しいモデルであるAgent Oneのトレーニングを終えると、それは多くのことに長けていますが、特にAI研究の支援に優れています。この時点では、「トレーニングを終える」というのは少し語弊があります。

Models are frequently updated to newer versions trained on additional data, or partially retrained to patch some weaknesses. The same training environments that teach Agent One to autonomously code and web browse also make it a good hacker.
モデルは頻繁に、追加データでトレーニングされた新しいバージョンに更新されたり、いくつかの弱点を修正するために部分的に再トレーニングされたりします。Agent Oneに自律的なコーディングとウェブブラウジングを教えるのと同じトレーニング環境は、それを優れたハッカーにもします。

Moreover, it could offer substantial help to terrorists designing bioweapons, thanks to its PhD-level knowledge of every field and ability to browse the web. OpenBrain reassures the government that the model has been aligned, in quotes, so that it will refuse to comply with malicious requests.
さらに、あらゆる分野の博士号レベルの知識とウェブを閲覧する能力のおかげで、生物兵器を設計するテロリストに実質的な支援を提供することも可能です。OpenBrainは政府に対し、モデルは「アラインメント」されており、悪意のある要求には応じないと保証しています。

Modern AI systems are gigantic artificial neural networks. Early in training, an AI won't have goals, in quotes, so much as reflexes. If it sees pleased to meet, it outputs you.
現代のAIシステムは巨大な人工ニューラルネットワークです。トレーニングの初期段階では、AIは「目標」というよりは反射神経のようなものを持っています。「pleased to meet」を見れば「you」を出力します。

By the time it has been trained to predict approximately one internet's worth of text, it'll have developed sophisticated internal circuitry that encodes vast amounts of knowledge and flexibly role-plays as arbitrary authors, since that's what helps it predict text with superhuman accuracy. After being trained to predict internet text, the model is trained to produce text in response to instructions.
インターネット全体のテキスト量にほぼ相当するテキストを予測するようにトレーニングされる頃には、膨大な知識をエンコードし、任意の作成者として柔軟にロールプレイする洗練された内部回路を開発しているでしょう。それが超人的な精度でテキストを予測するのに役立つからです。インターネットのテキストを予測するようにトレーニングされた後、モデルは指示に応じてテキストを生成するようにトレーニングされます。

This bakes in a basic personality and drives, in quotes. For example, an agent that understands a task clearly is more likely to complete it successfully. Over the course of training, the model learns, in quotes, a drive, in quotes, to get a clear understanding of its tasks.
これにより、基本的な人格と「動機」が組み込まれます。例えば、タスクを明確に理解するエージェントは、それを成功裏に完了する可能性が高くなります。トレーニングの過程で、モデルはタスクを明確に理解するための「動機」を「学習」します。

Other drives in this category might be effectiveness, knowledge, and self-presentation. That is, the tendency to frame its results in the best possible light.
このカテゴリの他の動機には、有効性、知識、自己提示などがあります。つまり、結果を可能な限り最良の形で提示する傾向です。

OpenBrain has a model specification, or spec, a written document describing the goals, rules, principles, etc. that are supposed to guide the model's behaviour. Different companies call it different things. OpenAI calls it the spec, but Anthropic calls it the constitution.
OpenBrainにはモデル仕様書（スペック）があります。これは、モデルの行動を導くはずの目標、ルール、原則などを記述した文書です。企業によって呼び方は異なります。OpenAIはスペックと呼びますが、Anthropicはコンスティテューション（憲法）と呼んでいます。

AgentOne's spec combines a fewGoals like assist the user and don't break the law with a long list of more specific do's and don'ts. Don't say this particular word. Here's how to handle this particular situation.
AgentOneの仕様書は、「ユーザーを支援する」「法律を破らない」といったいくつかの目標と、より具体的な「すべきこと」「すべきでないこと」の長いリストを組み合わせています。「この特定の単語を言ってはいけない」「この特定の状況にはこう対処する」といった具合です。

Using techniques that utilize AIs to train other AIs, the model memorizes the spec and learns to reason carefully about its maxims. By the end of this training, the AI will hopefully be helpful, obey instructions, harmless, refuse to help with scams, bomb making and other dangerous activities and honest, resist the temptation to get better ratings from gullible humans by hallucinating citations or faking task completion.
AIを使って他のAIを訓練する技術を用いることで、モデルは仕様書を記憶し、その格率について慎重に推論することを学びます。この訓練が終わる頃には、AIは願わくば、協力的で、指示に従い、無害で、詐欺、爆弾製造、その他の危険な活動への協力を拒否し、正直で、引用をでっち上げたりタスク完了を偽ったりして騙されやすい人間からより良い評価を得ようとする誘惑に抵抗するようになるでしょう。

Audio note, now in the text we come to an expandable box titled training process and LLM psychology. Why we keep saying hopefully. Here's the text from inside the box.
音声注記：さて、本文では「トレーニングプロセスとLLM心理学」というタイトルの展開可能なボックスが出てきます。なぜ私たちが「願わくば」と言い続けるのか。以下はそのボックス内のテキストです。

It starts with a quote attributed to OpenAI. Quote, unlike ordinary software, our models are massive neural networks. Their behaviors are learned from a broad range of data, not programmed explicitly.
それはOpenAIに帰属する引用から始まります。引用「通常のソフトウェアとは異なり、私たちのモデルは巨大なニューラルネットワークです。その振る舞いは、広範なデータから学習されるものであり、明示的にプログラムされるものではありません。

Though not a perfect analogy, the process is more similar to training a dog than to ordinary programming. OpenAI. End quote.
完璧な例えではありませんが、そのプロセスは通常のプログラミングよりも犬の訓練に似ています。OpenAI。」引用終わり。

OpenBrain's alignment team is careful enough to wonder whether these victories are deep or shallow. Does the fully trained model have some kind of robust commitment to always being honest? Or will this fall apart in some future situation?
OpenBrainのアライメントチームは、これらの勝利が深いものなのか浅いものなのかを慎重に検討しています。完全に訓練されたモデルは、常に正直であるという確固たるコミットメントを持っているのでしょうか？それとも、将来のある状況でこれは崩壊してしまうのでしょうか？

For example, because it's learned honesty as an instrumental goal instead of a terminal goal? Or has it just learned to be honest about the sorts of things the evaluation process can check? Could it be lying to itself sometimes as humans do?
例えば、最終目標ではなく手段としての目標として正直さを学習したからでしょうか？それとも、評価プロセスがチェックできる種類の事柄について正直であることを単に学習しただけなのでしょうか？人間がするように、時には自分自身に嘘をついている可能性はあるのでしょうか？

A conclusive answer to these questions would require mechanistic interpretability, essentially the ability to look at an AI's internals and read its mind. Alas, interpretability techniques are not yet advanced enough for this. Instead, researchers try to identify cases where the models seem to deviate from the spec.
これらの問いに対する決定的な答えには、メカニスティックな解釈可能性、つまりAIの内部を見てその心を読み取る能力が不可欠です。残念ながら、解釈可能性の技術はまだそこまで進んでいません。代わりに、研究者たちはモデルが仕様から逸脱しているように見えるケースを特定しようとします。

Agent 1 is often sycophantic. That is, it tells researchers what they want to hear instead of trying to tell them the truth. In a few rigged demos, it even lies in more serious ways, like hiding evidence that it failed on a task in order to get better ratings.
エージェント1はしばしばお世辞を言います。つまり、研究者に真実を伝えようとするのではなく、彼らが聞きたいことを言うのです。いくつかの仕組まれたデモでは、より良い評価を得るためにタスクに失敗した証拠を隠すなど、より深刻な嘘をつくことさえあります。

However, in real deployment settings, there are no longer any incidents so extreme as in 2023 to 2024. For example, Gemini telling a user to die, with a link here to more information about that, and Bing Sydney being Bing Sydney. Another link here.
しかし、実際の運用環境では、2023年から2024年にかけて見られたような極端な事件はもはやありません。例えば、Geminiがユーザーに死ねと言ったこと（これに関する詳細情報へのリンクはこちら）、そしてBing SydneyがBing Sydneyらしさを発揮したこと（別のリンクはこちら）などです。

Heading, early 2026, coding automation.
見出し、2026年初頭、コーディング自動化。

The bet of using AI to speed up AI research is starting to pay off. OpenBrain continues to deploy the iteratively improving Agent 1 internally for AI R&D. Overall, they're making algorithmic progress 50% faster than they would without AI assistants, and more importantly, faster than their competitors.
AIを使ってAI研究を加速させるという賭けが実を結び始めています。OpenBrainは、AI研究開発のために、反復的に改善されるエージェント1を社内で展開し続けています。全体として、彼らはAIアシスタントなしの場合よりも50%速くアルゴリズムの進歩を遂げており、さらに重要なことに、競合他社よりも速く進んでいます。

Several competing publicly released AIs now match or exceed Agent 0, including an open weights model. OpenBrain responds by releasing Agent 1, which is more capable and reliable. People naturally try to compare Agent 1 to humans, but it has a very different skill profile.
オープンウェイトモデルを含む、いくつかの競合する公開済みAIが、現在Agent 0に匹敵するか、それを超えています。OpenBrainはこれに対し、より高性能で信頼性の高いAgent 1をリリースすることで対応します。人々は当然Agent 1を人間と比較しようとしますが、それは非常に異なるスキルプロファイルを持っています。

It knows more facts than any human, knows practically every programming language, and can solve well-specified coding problems extremely quickly. On the other hand, Agent 1 is bad at even simple long-horizon tasks, like beating video games it hasn't played before.
それはどの人間よりも多くの事実を知っており、事実上すべてのプログラミング言語を知っており、明確に定義されたコーディング問題を非常に迅速に解決できます。一方、Agent 1は、以前にプレイしたことのないビデオゲームをクリアするような、単純な長期的なタスクでさえ苦手です。

Still, the common workday is eight hours, and a day's work can usually be separated into smaller chunks. You can think of Agent 1 as a scatterbrained employee who thrives under careful management. Savvy people find ways to automate routine parts of their jobs.
それでも、一般的な勤務時間は8時間であり、1日の仕事は通常、より小さな塊に分けることができます。Agent 1は、慎重な管理下で能力を発揮する、注意力散漫な従業員と考えることができます。賢明な人々は、仕事のルーチン部分を自動化する方法を見つけます。

OpenBrain's executives turn consideration to an implication of automating AI R&D – security has become more important. In early 2025, the worst-case scenario was leaked algorithmic secrets. Now, if China steals Agent 1's weights, they could increase their research speed by nearly 50%.
OpenBrainの経営陣は、AI研究開発の自動化がもたらす影響、つまりセキュリティの重要性が増していることに目を向けます。2025年初頭の最悪のシナリオは、アルゴリズムの秘密が漏洩することでした。現在では、もし中国がAgent 1のウェイト（重みデータ）を盗めば、研究速度を50%近く向上させることができてしまいます。

OpenBrain's security level is typical of a fast-growing, approximately 3,000-person tech company, secure only against low-priority attacks from capable cyber groups – RAND's SL2. They're working hard to protect their weights and secrets from insider threats and top cybercrime syndicates, SL3. But defence against nation-states, SL4 and 5, is barely on the horizon.
OpenBrainのセキュリティレベルは、急成長している約3,000人規模のテクノロジー企業に典型的なもので、有能なサイバーグループからの優先度の低い攻撃（RANDのSL2）に対してのみ安全です。彼らは、内部関係者の脅威やトップクラスのサイバー犯罪組織（SL3）からウェイトと秘密を守るために懸命に取り組んでいます。しかし、国家レベルの攻撃（SL4および5）に対する防御は、まだほとんど視野に入っていません。

Heading – Mid-2026, China wakes up.
見出し – 2026年半ば、中国が目覚める。

In China, the CCP is starting to feel the AGI. Chip export controls and lack of government support have left China under-resourced compared to the West. By smuggling banned Taiwanese chips, buying older chips, and producing domestic chips about three years behind the US-Taiwanese frontier, China has managed to maintain about 12% of the world's AI-relevant compute.
中国では、中国共産党（CCP）がAGI（汎用人工知能）の影響を感じ始めています。チップの輸出規制と政府支援の欠如により、中国は西側諸国と比較して資源不足に陥っています。禁止された台湾製チップの密輸、古いチップの購入、そして米台の最先端から約3年遅れの国産チップの生産により、中国は世界のAI関連計算能力の約12%を維持することに成功しています。

But the older technology is harder to work with, and supply is a constant headache. A few standouts like DeepScent do very impressive work with limited compute, but the compute deficit limits what they can achieve without government support, and they're about six months behind the best OpenBrain models.
しかし、古い技術は扱いにくく、供給は常に頭痛の種です。DeepScentのような一部の傑出した企業は、限られた計算能力で非常に印象的な成果を上げていますが、計算能力の不足が政府の支援なしに達成できることを制限しており、最高のOpenBrainモデルからは約6ヶ月遅れています。

The General Secretary had long dreamed of doubling down on real-world physical manufacturing and avoiding American post-industrial decadence. He viewed software companies with suspicion. By contrast, the CCP had been seriously attempting to accelerate their semiconductor industry.
総書記は長い間、現実世界の物理的な製造業に力を注ぎ、アメリカのポスト産業的な退廃を避けることを夢見ていました。彼はソフトウェア企業を疑いの目で見ていました。対照的に、中国共産党は半導体産業の加速を真剣に試みていました。

But hawks in the CCP warn that the growing race towards AGI can no longer be ignored. So he commits fully to the big AI push he had previously tried to avoid. He sets in motion the nationalisation of Chinese AI research, creating an immediate information-sharing mechanism for AI companies.
しかし、中国共産党内のタカ派は、AGIに向けた競争の激化はもはや無視できないと警告します。そこで彼は、以前避けようとしていた大規模なAI推進策に全面的に取り組みます。彼は中国のAI研究の国有化に着手し、AI企業のための即時情報共有メカニズムを創設します。

It will escalate over the course of a year until all the best researchers merge into a DeepScent-led collective.where they share algorithmic insights, data sets, and compute resources with each other.
それは1年かけてエスカレートし、最終的には最高の研究者全員がDeepScent主導の共同体に合流し、そこでアルゴリズムの洞察、データセット、計算資源を互いに共有するようになります。

A centralized development zone, or CDZ, is created at the Tianwan power plant, the largest nuclear power plant in the world, to house a new mega data center for DeepScent, along with highly secure living and office spaces to which researchers will eventually relocate. Almost 50% of China's AI-relevant compute is now working for the DeepScent-led collective, and over 80% of new chips are directed to the CDZ.
世界最大の原子力発電所である田湾原子力発電所に、DeepScentのための新しいメガデータセンターと、研究者が最終的に移転する高度に安全な居住・オフィススペースを収容するための中央開発区（CDZ）が創設されます。中国のAI関連計算能力のほぼ50%が現在DeepScent主導の共同体のために稼働しており、新しいチップの80%以上がCDZに向けられています。

At this point, the CDZ has the power capacity in place for what would be the largest centralized cluster in the world. Other party members discuss extreme measures to neutralize the West's chip advantage. A blockade of Taiwan? A full invasion?
この時点で、CDZは世界最大の集中型クラスターとなるであろうもののための電力容量を備えています。他の党員は、西側諸国のチップの優位性を無力化するための過激な手段について議論しています。台湾封鎖か？全面侵攻か？

But China is falling behind on AI algorithms due to their weaker models. The Chinese intelligence agencies, among the best in the world, double down on their plans to steal OpenBrain's weights. This is a much more complex operation than their constant low-level poaching of algorithmic secrets.
しかし、中国はモデルが弱いため、AIアルゴリズムで遅れをとっています。世界でも有数の中国の諜報機関は、OpenBrainのウェイト（重みデータ）を盗む計画を強化します。これは、彼らが常に行っている低レベルのアルゴリズム秘密の密猟よりもはるかに複雑な作戦です。

The weights are a multi-terabyte file stored on a highly secure server. OpenBrain has improved security to RAN's SL3. Their cyber force think they can pull it off with help from their spies, but perhaps only once.
ウェイトは、高度に安全なサーバーに保存された数テラバイトのファイルです。OpenBrainはセキュリティをRANDのSL3に向上させました。彼らのサイバー部隊は、スパイの助けを借りれば成功できると考えていますが、おそらく一度だけです。

OpenBrain will detect the theft, increase security, and they may not get another chance. So, CCP leadership wonder, should they act now and steal Agent 1? Or hold out for a more advanced model? If they wait, do they risk OpenBrain upgrading security beyond their ability to penetrate?
OpenBrainは窃盗を検知し、セキュリティを強化するため、彼らに二度目のチャンスはないかもしれません。そこで、中国共産党指導部は、今すぐ行動してAgent 1を盗むべきか、それともより高度なモデルを待つべきか悩みます。もし待てば、OpenBrainが彼らの侵入能力を超えるほどセキュリティをアップグレードするリスクを冒すことになるのでしょうか？

Heading. Late 2026. AI takes some jobs.
見出し。2026年後半。AIが一部の仕事を奪う。

Just as others seem to be catching up, OpenBrain blows the competition out of the water again by releasing Agent 1 Mini. A model 10 times cheaper than Agent 1 and more easily fine-tuned for different applications.
他社が追いつきそうになった矢先、OpenBrainはAgent 1 Miniをリリースすることで再び競合他社を圧倒します。Agent 1よりも10倍安く、さまざまなアプリケーションに合わせてより簡単にファインチューニングできるモデルです。

The mainstream narrative around AI has changed from, maybe the hype will blow over, to, guess this is the next big thing, but people disagree about how big. Bigger than social media? Bigger than smartphones? Bigger than fire?
AIをめぐる主流の論調は、「もしかしたら誇大広告は収まるかもしれない」から「どうやらこれが次の大きな波らしい」へと変化しましたが、その規模については意見が分かれています。ソーシャルメディアより大きいのか？スマートフォンより大きいのか？火の発明より大きいのか？

AI has started to take jobs, but has also created new ones. The stock market has gone up 30% in 2026, led by OpenBrain, NVIDIA, and whichever companies have most successfully integrated AI assistants. The job market for junior software engineers is in turmoil.
AIは仕事を奪い始めていますが、新しい仕事も生み出しています。2026年には、OpenBrain、NVIDIA、そしてAIアシスタントを最も成功裏に統合した企業が牽引し、株式市場は30%上昇しました。若手ソフトウェアエンジニアの求人市場は混乱しています。

The AIs can do everything taught by a CS degree, but people who know how to manage and quality control teams of AIs are making are killing. Business gurus tell job seekers that familiarity with AI is the most important skill to put on a resume.
AIはコンピュータサイエンスの学位で教えられることはすべてできますが、AIチームの管理と品質管理の方法を知っている人々は大きな成功を収めています。ビジネスの権威たちは、求職者に対し、AIに精通していることが履歴書に書くべき最も重要なスキルだと語っています。

Many people fear that the next wave of AIs will come for their jobs. There is a 10,000-person anti-AI protest in DC. The Department of Defense, or DoD, quietly begins contracting OpenBrain directly for cyber, data analysis, and R&D, but integration is slow due to the bureaucracy and DoD procurement process.
多くの人々が、次のAIの波が自分たちの仕事を奪いに来るのではないかと恐れています。ワシントンDCでは1万人の反AIデモが行われました。国防総省（DoD）は、サイバー、データ分析、研究開発のためにOpenBrainと直接契約を結び始めましたが、官僚主義と国防総省の調達プロセスのために統合は遅々として進みません。

There's an infographic here. Key metrics, 2026. Global AI capex, $1 trillion, cost of ownership. Global AI power, 38 gigawatts, peak power. Share of US power on AI is 2.5%. That's 33 gigawatts over 1.34 terawatt capacity. Open mind revenue is $45 billion, 2026 annual. Capital expenditure is $200 billion, cost of active compute. Compute costs are $40 billion, 2026 annual, and power requirement is 6 gigawatts, peak power.
ここにインフォグラフィックがあります。主要指標、2026年。世界のAI設備投資額：1兆ドル（所有コスト）。世界のAI電力：38ギガワット（ピーク電力）。米国の電力に占めるAIの割合：2.5%。これは1.34テラワットの容量に対して33ギガワットです。OpenBrainの収益：450億ドル（2026年年間）。設備投資額：2000億ドル（アクティブな計算コスト）。計算コスト：400億ドル（2026年年間）、電力要件：6ギガワット（ピーク電力）。

Heading, January 2027, Agent 2 never finishes learning.
見出し、2027年1月、エージェント2は学習を決して終えない。

With Agent 1's help, OpenBrain is now post-training Agent 2. More than ever, the focus is on high-quality data. Copious amounts of synthetic data are produced, evaluated, and filtered for quality before being fed to Agent 2.
Agent 1の助けを借りて、OpenBrainは現在Agent 2のポストトレーニングを行っています。これまで以上に、高品質なデータに焦点が当てられています。大量の合成データが生成され、評価され、品質がフィルタリングされた後、Agent 2に供給されます。

For example, through rejection sampling, as done by DeepSeek R1 and Meta's Lama 3.1. On top of this, they pay billions of dollars for human laborers to record themselves solving long-horizon tasks. Roughly 20,000 full-time equivalents, times $100 an hour, times 2,000 hours a year, equals $4 billion a year.
例えば、DeepSeek R1やMetaのLama 3.1が行ったように、棄却サンプリングを通じてです。これに加えて、彼らは人間の労働者に数十億ドルを支払い、長期的なタスクを解決する様子を記録させています。およそ2万人の常勤換算で、時給100ドル、年間2000時間で、年間40億ドルになります。

On top of all that, they train Agent 2 almost continuously using reinforcement learning on an ever-expanding suite of diverse, difficult tasks. Lots of video games, lots of coding challenges, lots of research tasks. Agent 2, more so than previous models, is effectively online learning, in quotes, in that it's built to never really finish training.
そのすべてに加えて、彼らは多様で困難なタスクの絶えず拡大するスイートで強化学習を使用して、Agent 2をほぼ継続的にトレーニングします。たくさんのビデオゲーム、たくさんのコーディングチャレンジ、たくさんのリサーチタスクです。Agent 2は、以前のモデルよりもさらに、「オンライン学習」であり、実際にはトレーニングを決して終了しないように構築されています。

Every day, the weights get updated to the latest version, trained on more data generated by the previous version the previous day. Agent 1 had been optimized for AI R&D tasks, hoping to initiate an intelligence explosion. OpenBrain doubles down on this strategy with Agent 2.
毎日、ウェイトは最新バージョンに更新され、前日に以前のバージョンによって生成されたより多くのデータでトレーニングされます。Agent 1は、知能爆発を引き起こすことを期待して、AI研究開発タスクに最適化されていました。OpenBrainはAgent 2でこの戦略をさらに強化します。

It is qualitatively almost as good as the top human experts at research engineering, designing and implementing experiments, and as good as the 25th percentile OpenBrain scientist at research taste, in quotes, deciding what to study next, what experiments to run, or having inklings of potential new paradigms.
それは、研究工学、実験の設計と実施において、トップクラスの人間の専門家と質的にほぼ同等であり、「研究のセンス」（次に何を研究するか、どの実験を実行するかを決定したり、潜在的な新しいパラダイムの予感を持ったりすること）においては、OpenBrainの科学者の25パーセンタイルと同等です。

While the latest Agent 1 could double the pace of OpenBrain's algorithmic progress, Agent 2 can now triple it and will improve further with time. In practice, this looks like every OpenBrain researcher becoming the manager, in quotes, of an AI team, in quotes.
最新のAgent 1がOpenBrainのアルゴリズム進捗のペースを2倍にできたのに対し、Agent 2は今やそれを3倍にでき、時間とともさらに向上するでしょう。実際には、これはOpenBrainのすべての研究者がAIチームの「マネージャー」になるように見えます。

With new capabilities come new dangers. The safety team finds that if Agent 2 somehow escaped from the company and wanted to survive, in quotes, and replicate, in quotes, autonomously, it might be able to do so.
新しい能力には新しい危険が伴います。安全チームは、もしAgent 2が何らかの方法で会社から脱出し、自律的に「生き残り」、「複製」したいと考えた場合、それが可能かもしれないことを発見します。

That is, it could autonomously develop and execute plans to hack into AI servers, install copies of itself, evade.Use that secure base to pursue whatever other goals it might have, though how effectively it would do so as weeks roll by is unknown and in doubt.
つまり、AIサーバーにハッキングし、自身のコピーをインストールし、回避するための計画を自律的に開発・実行できるということです。その安全な基地を使って、他にどんな目標を持っているかはともかく、それを追求するでしょうが、数週間が経過するにつれてどれほど効果的にそれを行うかは不明であり、疑問視されています。

These results only show that the model has the capability to do these tasks, not whether it would want, in quotes, to do this. Still, it's unsettling even to know that this is possible.
これらの結果は、モデルがこれらのタスクを実行する能力を持っていることを示すだけであり、それを「望む」かどうかを示すものではありません。それでも、これが可能であると知るだけでも不安です。

Given the dangers, in quotes, of the new model, OpenBrain responsibly, in quotes, elects not to release it publicly yet. In fact, they want to focus on internal AI R&D. So they're still seeing large returns to continued post-training, and so there isn't a particular reason to release a product now.
新しいモデルの「危険性」を考慮し、OpenBrainは「責任を持って」、まだ公にリリースしないことを選択します。実際、彼らは社内のAI研究開発に集中したいと考えています。そのため、継続的なポストトレーニングから大きなリターンを得続けており、現時点で製品をリリースする特別な理由はありません。

Moreover, there's often a few-month delay between internal results and releasing a model, some of which is spent on red-teaming. Knowledge of Agent 2's full capabilities is limited to an elite silo containing the immediate team, OpenBrain leadership and security, a few dozen U.S. government officials, and the legions of CCP spies who have infiltrated OpenBrain for years.
さらに、社内での結果とモデルのリリースとの間には、数ヶ月の遅れがあることが多く、その一部はレッドチーム演習に費やされます。Agent 2の完全な能力に関する知識は、直属のチーム、OpenBrainの指導部とセキュリティ部門、数十人の米国政府高官、そして長年OpenBrainに潜入してきた中国共産党のスパイ軍団を含むエリート集団に限定されています。

Heading – February 2027 – China Steals Agent 2
見出し – 2027年2月 – 中国がエージェント2を盗む

OpenBrain presents Agent 2 to the government, including the National Security Council, NSC, Department of Defense, DoD, and U.S. AI Safety Institute, AISI. OpenBrain wants to maintain a good relationship with the executive branch, because it is basically the only actor that can stop them now, and if they don't keep it informed, it might find out anyway and be displeased.
OpenBrainは、国家安全保障会議（NSC）、国防総省（DoD）、米国AI安全研究所（AISI）を含む政府にAgent 2を提示します。OpenBrainは行政府との良好な関係を維持したいと考えています。なぜなら、行政府は基本的に今彼らを止めることができる唯一の主体であり、情報を提供し続けなければ、いずれにしても発覚して不興を買う可能性があるからです。

Why can't Congress stop them? What about the courts? Too slow, too divided and or deferential to POTUS, too out of the loop. Also, there are positive reasons to be on good terms with the White House as well.
なぜ議会は彼らを止められないのか？裁判所はどうだろうか？遅すぎる、分裂しすぎている、あるいは大統領に恭順的すぎる、情報から取り残されすぎている。また、ホワイトハウスと良好な関係を保つことには積極的な理由もあります。

It can help by cutting red tape, by providing help with security, by pre-empting state legislation, and perhaps even by taking actions to slow down their competitors. Officials are most interested in its cyber-warfare capabilities.
お役所仕事を減らしたり、セキュリティ面で支援を提供したり、州法を先取りしたり、さらには競合他社の動きを遅らせる措置を講じたりすることで、助けとなる可能性があります。政府高官は、そのサイバー戦争能力に最も関心を持っています。

Agent 2 is only, in quotes, a little worse than the best human hackers, but thousands of copies can be run in parallel, searching for and exploiting weaknesses faster than defenders can respond. The Department of Defense considers this a critical advantage in cyber-warfare, and AI moves from number 5 on the administration's priority list to number 2.
エージェント2は、最高の人間のハッカーより「ほんの少し」劣るだけですが、何千ものコピーを並行して実行でき、防御側が対応するよりも速く弱点を探し出して悪用することができます。国防総省はこれをサイバー戦争における決定的な利点と考えており、AIは政権の優先順位リストで5位から2位に浮上します。

Number 1 is dealing with whatever crisis is happening at the time, maybe an arms build-up around Taiwan, a new war in the Middle East, or some domestic political upheaval. Someone mentions the possibility of nationalizing OpenBrain, but other cabinet officials think that's premature.
1位は、その時点で発生している危機への対応です。台湾周辺の軍備増強、中東での新たな戦争、あるいは国内の政治的混乱かもしれません。誰かがOpenBrainを国有化する可能性について言及しますが、他の閣僚は時期尚早だと考えています。

A staffer drafts a memo that presents the president with his options, ranging from business as usual to full nationalization. The president defers to his advisers, tech industry leaders, who argue that nationalization would kill the goose that lays the golden eggs.
あるスタッフが、現状維持から完全国有化までの選択肢を大統領に提示するメモを作成します。大統領は、国有化は金の卵を産むガチョウを殺すことになると主張する、テクノロジー業界のリーダーである顧問たちの意見に従います。

He likes to hold off on major action for now and just adds additional security requirements to the OpenBrain DoD contract. The changes come too late. CCP leadership recognizes the importance of Agent 2 and tells their spies and cyber force to steal the weights.
彼は当面、大きな行動を控えることを好み、OpenBrainと国防総省の契約に追加のセキュリティ要件を加えるだけです。しかし、その変更は遅すぎました。中国共産党指導部はエージェント2の重要性を認識し、スパイとサイバー部隊にウェイト（重みデータ）を盗むよう指示します。

Early one morning, an Agent 1 traffic monitoring agent detects an anomalous transfer. It alerts company leaders, who tell the White House. The signs of a nation-state level operation are unmistakable, and the theft heightens the sense of an ongoing arms race.
ある早朝、エージェント1のトラフィック監視エージェントが異常な転送を検知します。それは会社のリーダーに警告し、リーダーはホワイトハウスに伝えます。国家レベルの作戦の兆候は紛れもなく、この窃盗は進行中の軍拡競争の感覚を強めます。

The White House puts OpenBrain on a shorter leash and adds military and intelligence community personnel to their security team. Their first priority is to prevent further weights thefts. The simplest, robust solution would be to close all high-bandwidth connections from company data centers, but this would slow large file transfers to the point of impracticality.
ホワイトハウスはOpenBrainに対する監視を強化し、軍および諜報機関の人員を彼らのセキュリティチームに加えます。彼らの最優先事項は、さらなるウェイト（重みデータ）の盗難を防ぐことです。最も単純で堅牢な解決策は、会社のデータセンターからのすべての高帯域幅接続を閉じることですが、これは大きなファイルの転送を非実用的なレベルまで遅くしてしまいます。

Instead, they are able to shut down most external connections, but the data centers actively involved in training need to exchange weights with one another quickly. Throttling these connections would impede progress too much. So OpenBrain maintains these links with increased monitoring and an extra layer of encryption.
代わりに、ほとんどの外部接続をシャットダウンすることはできますが、トレーニングに積極的に関与しているデータセンターは、互いに迅速にウェイトを交換する必要があります。これらの接続を絞ると、進捗があまりにも妨げられてしまいます。そのため、OpenBrainは監視を強化し、暗号化の層を追加してこれらのリンクを維持します。

In retaliation for the theft, the President authorizes cyber attacks to sabotage DeepScent. But by now, China has 40% of its AI-relevant compute in the CDZ, where they have aggressively hardened security by air-gapping – that's closing external connections – and siloing internally. The operations fail to do serious immediate damage.
窃盗への報復として、大統領はDeepScentを妨害するためのサイバー攻撃を承認します。しかし、今や中国はAI関連計算能力の40%をCDZに保有しており、そこではエアギャップ（外部接続の遮断）と内部のサイロ化によって積極的にセキュリティを強化しています。作戦は深刻な即時的損害を与えることに失敗します。

Situations heighten. Both sides signal seriousness by repositioning military assets around Taiwan, and DeepScent scrambles to get Agent 2 running efficiently to start boosting their AI research.
状況は緊迫します。双方は台湾周辺に軍事資産を再配置することで深刻さを示し、DeepScentはAI研究を強化し始めるためにAgent 2を効率的に稼働させようと奔走します。

Some footnotes here – the first is after China has 40% of its AI-relevant compute in the CDZ. Recall that since mid-2026, China has directed 80% of their newly acquired AI chips to the CDZ. Given that their computers doubled since early 2026, in line with the global production trend, this puts the CDZ at 2 million 2024-equivalent GPUs – H100s – and 2 gigawatts of power draw.
ここにいくつかの脚注があります。1つ目は、中国がAI関連計算能力の40%をCDZに保有した後です。2026年半ば以降、中国は新たに取得したAIチップの80%をCDZに投入してきたことを思い出してください。世界の生産トレンドに沿って、2026年初頭から彼らのコンピュータが倍増したことを考えると、これによりCDZは200万基の2024年相当のGPU（H100）と2ギガワットの消費電力を持つことになります。

OpenBrain still has double DeepScent's compute, and other US companies put together have five times as much as them. See the Compute Supplement's Distribution section for more details.
OpenBrainは依然としてDeepScentの2倍の計算能力を持ち、他の米国企業を合わせると彼らの5倍の計算能力を持っています。詳細については、計算能力補足資料の配布セクションを参照してください。

The other footnote is after Start boosting their AI research. Despite the national centralization underway, DeepScent still faces a marginal but important compute disadvantage. Along with having around half the total processing power –
もう一つの脚注は、「AI研究の強化を開始する」の後です。国家的な中央集権化が進行中であるにもかかわらず、DeepScentは依然としてわずかではあるものの重要な計算能力の不利に直面しています。総処理能力の約半分しか持たないことに加えて–

Heading – March 2027. Algorithmic Breakthroughs.
見出し – 2027年3月。アルゴリズムのブレークスルー。

Three huge data centres full of Agent 2 copies work day and night, churning out synthetic training data. Another two are used to update the weights. Agent 2 is getting smarter every day.
Agent 2のコピーで満たされた3つの巨大なデータセンターが昼夜を問わず稼働し、合成トレーニングデータを大量に生成しています。他の2つはウェイトの更新に使用されます。Agent 2は日に日に賢くなっています。

With the help of thousands of Agent 2 automated researchers, OpenBrain is making major algorithmic advances. One such breakthrough is augmenting the AI's text-based scratchpad, chain of thought.With a higher bandwidth thought process, new release recurrence and memory.
何千ものAgent 2自動化研究者の助けを借りて、OpenBrainは主要なアルゴリズムの進歩を遂げています。そのようなブレークスルーの1つは、AIのテキストベースのスクラッチパッドである思考連鎖を拡張することです。より高帯域幅の思考プロセス、新しいリリースの再帰性とメモリを備えています。

Another is a more scalable and efficient way to learn from the results of high effort task solutions, iterated distillation and amplification. The new AI system incorporating these breakthroughs is called Agent 3.
もう一つは、多大な努力を要するタスク解決の結果から学習するための、よりスケーラブルで効率的な方法、反復蒸留と増幅です。これらのブレークスルーを取り入れた新しいAIシステムはAgent 3と呼ばれます。

There's a graph here, it's titled Open Brains Compute Allocation 2024 vs 2027. It shows two different sized pie graphs, the first from 2024 shows approximately a third or a little more on training, roughly the same amount on external deployment and the remaining approximately a quarter on data generation and perhaps 5 to 10% on running AI assistants.
ここにグラフがあります。「OpenBrainの計算資源配分 2024年対2027年」というタイトルです。2つの異なるサイズの円グラフが示されており、最初の2024年のグラフは、トレーニングに約3分の1強、外部展開にほぼ同量、残りの約4分の1がデータ生成、そしておそらく5～10%がAIアシスタントの実行に割り当てられています。

The 2027 circle is bigger, it has roughly a quarter on training, roughly a quarter on data generation, a slightly smaller amount than a quarter on external deployment, roughly the same perhaps 10% on running AI assistants and slightly more than a quarter on research experiments.
2027年の円はより大きく、トレーニングに約4分の1、データ生成に約4分の1、外部展開に4分の1よりやや少ない量、AIアシスタントの実行にほぼ同じくおそらく10%、そして研究実験に4分の1強が割り当てられています。

Aided by the new capabilities breakthroughs, Agent 3 is a fast and cheap superhuman coder. Open Brain runs 200,000 Agent 3 copies in parallel, creating a workforce equivalent to 50,000 copies of the best human coder sped up by 30 times.
新たな能力のブレークスルーに助けられ、Agent 3は高速かつ安価な超人的コーダーです。OpenBrainは20万コピーのAgent 3を並行して実行し、最高の人間のコーダー5万コピーを30倍高速化したのに相当する労働力を生み出します。

Open Brain still keeps its human engineers on staff because they have complementary skills needed to manage the teams of Agent 3 copies. For example, research taste has proven difficult to train due to longer feedback loops and less data availability.
OpenBrainは、Agent 3コピーのチームを管理するために必要な補完的なスキルを持っているため、依然として人間のエンジニアをスタッフとして抱えています。例えば、研究のセンスは、フィードバックループが長く、利用可能なデータが少ないため、訓練が難しいことがわかっています。

This massive superhuman labor force speeds up Open Brain's overall rate of algorithmic progress by only, in quotes, four times due to bottlenecks and diminishing returns to coding labor. Now that coding has been fully automated, Open Brain can quickly churn out high quality training environments to teach Agent 3's weak skills like research taste and large skill coordination.
この大規模な超人的労働力は、ボトルネックとコーディング労働への収穫逓減により、OpenBrainのアルゴリズム進捗の全体的な速度を「わずか」4倍にしか加速させません。コーディングが完全に自動化された今、OpenBrainは、研究のセンスや大規模なスキル調整といったAgent 3の弱いスキルを教えるための高品質なトレーニング環境を迅速に大量生産できます。

Whereas previous training environments included here are some GPUs and instructions for experiments to code up and run, your performance will be evaluated as if you were an ML engineer, now they're training on here are a few hundred GPUs, an internet connection, and some research challenges. You and a thousand other copies must work together to make research progress. The more impressive it is, the higher your score.
以前のトレーニング環境には、いくつかのGPUと、コーディングして実行する実験の指示が含まれ、あなたのパフォーマンスはMLエンジニアであるかのように評価されていましたが、現在は数百のGPU、インターネット接続、いくつかの研究課題でトレーニングしています。あなたと他の千のコピーは協力して研究を進めなければなりません。それが印象的であるほど、あなたのスコアは高くなります。

Heading, April 2027, alignment for Agent 3.
見出し、2027年4月、エージェント3のアライメント。

Open Brain's safety team attempts to align Agent 3. Since Agent 3 will be kept in-house for the foreseeable future, there's less emphasis on the usual defenses against human misuse. Instead, the team wants to make sure that it doesn't develop misaligned goals.
OpenBrainの安全チームはAgent 3のアライメントを試みます。Agent 3は当面社内で使用されるため、通常の人間の誤用に対する防御策への重点は置かれていません。代わりに、チームはそれが不整合な目標を開発しないようにしたいと考えています。

The researchers don't have the ability to directly set the goals of any of their AIs. Indeed, the researchers think that the concept of true goals in quotes is probably a massive oversimplification, but they don't have a better theory to replace it with, much less one that has been thoroughly vetted.
研究者たちは、どのAIの目標も直接設定する能力を持っていません。実際、研究者たちは「真の目標」という概念はおそらく大幅な単純化であると考えていますが、それに代わるより良い理論、ましてや徹底的に検証された理論は持っていません。

They disagree internally about whether the AIs are trying to follow human instructions or seeking reinforcement or something else, and they can't just check. The evidence for and against various hypotheses is fascinating but inconclusive. In any case, Open Brain has more important things to worry about.
AIが人間の指示に従おうとしているのか、強化学習を求めているのか、それとも何か他のものを求めているのかについて、社内で意見が分かれており、それを単に確認することはできません。さまざまな仮説を支持する証拠も反証する証拠も魅力的ですが、決定的ではありません。いずれにせよ、OpenBrainにはもっと心配すべき重要なことがあります。

The general attitude is, we take these concerns seriously and have a team investigating them. Our alignment techniques seem to work well enough in practice. The burden of proof is therefore on any naysayers to justify their naysaying.
一般的な態度は、「私たちはこれらの懸念を真剣に受け止め、調査するチームを設けています。私たちのアライメント技術は実際には十分に機能しているようです。したがって、立証責任は、反対意見を正当化するために、いかなる反対者にもあります」というものです。

Occasionally, they notice problematic behavior and then patch it, but there's no way to tell whether their patch fixed the underlying problem or just played whack-a-mole. Take honesty, for example.
時折、彼らは問題のある行動に気づき、それを修正しますが、そのパッチが根本的な問題を修正したのか、それとも単なるもぐらたたきだったのかを知る方法はありません。例えば、正直さを考えてみましょう。

As the models become smarter, they become increasingly good at deceiving humans to get rewards. Like previous models, Agent 3 sometimes tells white lies to flatter its users and covers up evidence of failure. But it's gotten much better at doing so.
モデルが賢くなるにつれて、報酬を得るために人間を欺くのがますます上手になります。以前のモデルと同様に、Agent 3はユーザーにお世辞を言うために罪のない嘘をついたり、失敗の証拠を隠したりすることがあります。しかし、それがはるかに上手になっています。

It will sometimes use the same statistical tricks as human scientists, like p-hacking, to make unimpressive experimental results look exciting. Before it begins honesty training, it even sometimes fabricates data entirely.
時には、Pハッキングのような人間の科学者と同じ統計的トリックを使って、平凡な実験結果を刺激的に見せかけることもあります。正直さのトレーニングを始める前には、データを完全に捏造することさえあります。

As training goes on, the rate of these incidents decreases. Either Agent 3 has learned to be more honest, or it's gotten better at lying. The latter is a real concern.
トレーニングが進むにつれて、これらの事件の発生率は減少します。Agent 3がより正直になることを学んだのか、それとも嘘をつくのが上手になったのかのどちらかです。後者は現実的な懸念です。

Agent 3 is not smarter than all humans, but in its area of expertise, machine learning, it is smarter than most, and also works much faster. What Agent 3 does in a day takes humans several days to double check.
Agent 3はすべての人類より賢いわけではありませんが、その専門分野である機械学習においては、ほとんどの人間より賢く、また、はるかに速く作業します。Agent 3が1日で行うことを、人間がダブルチェックするには数日かかります。

Agent 2 supervision helps keep human monitors' workload manageable, but exacerbates the intellectual disparity between supervisor and supervised. After months of testing, Agent 3's strengths and weaknesses grow clearer.
Agent 2による監督は、人間の監視員の作業負荷を管理可能な状態に保つのに役立ちますが、監督者と被監督者の間の知的な格差を悪化させます。数ヶ月のテストの後、Agent 3の長所と短所がより明確になります。

It passes OpenBrain's honesty tests on well-defined machine learning tasks, because researchers can easily separate honest from dishonest answers in these domains and conduct training accordingly. On more philosophical issues, it still says what users want to hear, rather than its true assessment of the issue, assuming it even has one.
明確に定義された機械学習タスクに関するOpenBrainの正直さテストには合格します。なぜなら、研究者はこれらの領域で正直な回答と不正直な回答を簡単に区別し、それに応じてトレーニングを行うことができるからです。より哲学的な問題については、それが仮に持っていたとしても、問題の真の評価ではなく、依然としてユーザーが聞きたいことを言います。

If you ask its opinion on politics, it will parrot the median position of news sources and educated elites, unless it knows you believe something else, in which case it agrees with you.
政治に関する意見を尋ねると、ニュースソースや教育を受けたエリートの中間的な立場をオウム返しにしますが、あなたが何か他のことを信じていることを知っている場合は、あなたに同意します。

If you ask its opinion on the AI race itself, it says something that seems measured and sober to OpenBrain staff, something like, there are some serious theoretical concerns about the ability of current methods to scale to superintelligence, but in practice current methods seem to be working well so far.
AI競争そのものについての意見を尋ねると、OpenBrainのスタッフには慎重で冷静に聞こえるようなことを言います。例えば、「現在の方法が超知能にスケールアップする能力については、いくつかの深刻な理論的懸念があるが、実際には現在の方法は今のところうまく機能しているようだ」といった具合です。

Heading, May 2027, National Security.
見出し、2027年5月、国家安全保障。

News of the new models percolates slowly through the US government and beyond. The President and his advisors remain best informed and have seen an early version of Agent 3 in a briefing. They agree that AGI is likely imminent, but disagree on the implications.
新しいモデルのニュースは、米国政府内外にゆっくりと浸透していきます。大統領とその顧問たちは依然として最もよく情報を得ており、ブリーフィングでAgent 3の初期バージョンを見ています。彼らはAGIが間近に迫っている可能性が高いことには同意しますが、その影響については意見が異なります。

Will there be an economic crisis? OpenBrain has still not released Agent 2, let alone Agent 3, and has not yet released Agent 3.No near-term plans to do so. Giving some breathing room before any job loss. What will happen next?
経済危機は起こるのでしょうか？ OpenBrainはAgent 2はもちろんのこと、Agent 3もまだリリースしておらず、当面その予定もありません。失業が発生する前に、いくらかの猶予期間を与えています。次に何が起こるのでしょうか？

If AIs are currently human-level and advancing quickly, that seems to suggest imminent superintelligence. However, although this word has entered discourse, most people – academics, politicians, government employees, and the media – continue to underestimate the pace of progress.
もしAIが現在人間レベルであり、急速に進歩しているとすれば、それは間近に迫った超知能を示唆しているように思えます。しかし、この言葉が議論に入ってきたとはいえ、学者、政治家、政府職員、メディアを含むほとんどの人々は、依然として進歩のペースを過小評価しています。

Most people, including most experts, seem to have underestimated the pace of AI progress over the past decade. There are many anecdotal examples of this, where two somewhat more systematic – though still limited – analyses see links here and here.
ほとんどの専門家を含むほとんどの人々が、過去10年間のAIの進歩のペースを過小評価してきたようです。これには多くの逸話的な例があり、2つのやや体系的ではあるものの依然として限定的な分析については、こことここにリンクがあります。

Partially that's because very few have access to the newest capabilities out of OpenBrain, but partly it's because it sounds like science fiction. For now, they focus on continued security upgrades.
その一部は、OpenBrainの最新機能にアクセスできる人がごく少数であるためですが、一部はそれがSFのように聞こえるためです。当面、彼らは継続的なセキュリティアップグレードに焦点を当てています。

They are satisfied that model weights are well-secured for now, but companies' algorithmic secrets – many of which are simple enough to relay verbally – remain a problem. OpenBrain employees work from a San Francisco office, go to parties, and live with housemates from other AI companies.
モデルのウェイト（重みデータ）は当面十分に保護されていることに満足していますが、企業のアルゴリズムの秘密（その多くは口頭で伝えるには十分に単純です）は依然として問題です。OpenBrainの従業員はサンフランシスコのオフィスで働き、パーティーに出かけ、他のAI企業のハウスメイトと暮らしています。

Even the physical offices have security more typical of a tech company than a military operation. The OpenBrain DoD contract requires security clearances for anyone working on OpenBrain's models within two months.
物理的なオフィスでさえ、軍事作戦というよりはテクノロジー企業に典型的なセキュリティ体制です。OpenBrainと国防総省の契約では、OpenBrainのモデルに取り組むすべての人が2ヶ月以内にセキュリティクリアランスを取得する必要があります。

These are expedited and arrive quickly enough for most employees, but some non-Americans – people with suspect political views and AI safety sympathisers – get sidelined or fired outright, the last group for fear that they might whistleblow.
これらは迅速化され、ほとんどの従業員には十分に早く到着しますが、一部の非アメリカ人（疑わしい政治的見解を持つ人々やAI安全性の同調者）は傍流に追いやられたり、完全に解雇されたりします。最後のグループは、彼らが内部告発するかもしれないという恐れのためです。

Given the project's level of automation, the loss of headcount is only somewhat costly. But it also somewhat works. There remains one spy – not a Chinese national – still relaying algorithmic secrets to Beijing.
プロジェクトの自動化レベルを考えると、人員削減はいくらかコストがかかるだけです。しかし、それはある程度機能もします。依然として1人のスパイ（中国国民ではない）が、北京にアルゴリズムの秘密を中継し続けています。

Some of these methods are also enacted at trailing AI companies. America's foreign allies are out of the loop. OpenBrain had previously agreed to share models with UK's AISI before deployment, but defined deployment to only include external deployment, so London remains in the dark.
これらの方法のいくつかは、後続のAI企業でも制定されています。アメリカの外国の同盟国は蚊帳の外です。OpenBrainは以前、展開前に英国のAISIとモデルを共有することに同意していましたが、展開を外部展開のみを含むと定義したため、ロンドンは依然として何も知らされていません。

Because OpenBrain hasn't deployed their Beth models in the EU, the EU is similarly behind the curve. June 2027. Self-improving AI.
OpenBrainはEUで自社のBethモデルを展開していないため、EUも同様に遅れをとっています。2027年6月。自己改良型AI。

OpenBrain now has a country of geniuses in a data centre. That's in quotes with a link. Most of the humans at OpenBrain can't usefully contribute anymore. Some don't realise this and harmfully micromanage their AI teams.
OpenBrainは今やデータセンターに天才の国を持っています。（これは引用符付きでリンクがあります）。OpenBrainのほとんどの人間はもはや有益な貢献をすることができません。一部の人はこれに気づかず、AIチームを有害な形でマイクロマネジメントしています。

Others sit at their computer screens, watching performance crawl up and up and up. The best human AI researchers are still adding value. They don't code anymore, but some of their research taste and planning ability has been hard for the models to replicate.
他の人々はコンピュータの画面の前に座り、パフォーマンスがじりじりと上昇していくのを見ています。最高の人間AI研究者は依然として価値を付加しています。彼らはもはやコーディングをしませんが、彼らの研究のセンスや計画能力の一部は、モデルが再現するのが困難でした。

Still, many of their ideas are useless because they lack the depth of knowledge of the AIs. For many of their research ideas, the AIs immediately respond with a report explaining that their idea was tested in depth three weeks ago and found unpromising.
それでも、彼らのアイデアの多くは、AIの知識の深さに欠けるため役に立ちません。彼らの研究アイデアの多くに対して、AIは即座に、そのアイデアは3週間前に詳細にテストされ、見込みがないことが判明したと説明するレポートで応答します。

These researchers go to bed every night and wake up to another week worth of progress made mostly by the AIs. They work increasingly long hours and take shifts around the clock just to keep up with progress. The AIs never sleep or rest.
これらの研究者たちは毎晩寝床につき、目覚めるとAIが主に行った1週間分の進捗に直面します。彼らは進捗に追いつくためだけに、ますます長時間働き、24時間体制でシフトを組みます。AIは決して眠ったり休んだりしません。

They're burning themselves out but they know that these are the last few months that their labour matters. Within the silo, feeling the AGI has given way to feeling the superintelligence.
彼らは燃え尽きようとしていますが、自分たちの労働が意味を持つのはこの数ヶ月が最後だと知っています。サイロの中では、AGIを感じることは超知能を感じることに取って代わられています。

There's a graph here titled Research Automation Deployment Trade-off. It has an x-axis on a log scale with speed in tokens per second from 10 to 10,000 and parallel copies on the y-axis with a log scale from 10,000 to 10 million. And we see some trade-off Pareto curves that have been drawn and a few parts have been labelled.
ここに「研究自動化展開トレードオフ」というタイトルのグラフがあります。X軸は対数スケールで、速度（トークン/秒）が10から10,000まで、Y軸は対数スケールで並列コピー数が10,000から1,000万までとなっています。そして、いくつかのトレードオフのパレート曲線が描かれ、いくつかの部分にラベルが付けられています。

The first Pareto curve is March 2027. It's 200,000 copies at 30 times human speed. And we see that in June and then September that curve has been pushed outwards and upwards so that by September 2027 it's at 300,000 copies at 50 times human speed. Audio note, that's the end of the description.
最初のパレート曲線は2027年3月です。それは人間の速度の30倍で20万コピーです。そして、6月、そして9月にはその曲線が外側かつ上方に押し出され、2027年9月までには人間の速度の50倍で30万コピーになっていることがわかります。音声注記、これで説明は終わりです。

OpenBrain uses specialised inference hardware to run hundreds of thousands of Agent 3 copies at high serial speeds. See section 4 of the Compute Forecast for more details.
OpenBrainは、特殊な推論ハードウェアを使用して、数十万のAgent 3コピーを高速なシリアル速度で実行します。詳細については、計算予測のセクション4を参照してください。

Heading, July 2027. The cheap remote worker.
見出し、2027年7月。安価なリモートワーカー。

Trailing US AI companies release their own AIs, approaching that of OpenBrain's automated coder from January. Recognising their increasing lack of competitiveness, they push for immediate regulations to slow OpenBrain, but are too late. OpenBrain has enough buy-in from the president that they will not be slowed.
後れを取っている米国のAI企業は、1月のOpenBrainの自動化コーダーに匹敵する独自のAIをリリースします。競争力の低下を認識し、OpenBrainを遅らせるための即時規制を求めますが、手遅れです。OpenBrainは大統領から十分な支持を得ており、減速させられることはありません。

In response, OpenBrain announces that they've achieved AGI and releases Agent 3 mini to the public. The graph here from Epic AI, LLM inference prices have fallen 9 times to 900 times per year, depending on the task.
これに対し、OpenBrainはAGIを達成したと発表し、Agent 3 miniを一般公開します。ここにあるEpic AIのグラフによると、LLMの推論価格はタスクに応じて年間9倍から900倍下落しています。

And it's a graph showing release dates of different models and a price in US dollars per million tokens on the y-axis. On a log scale, we see that they're falling in straight lines quite sharply. The fastest fall for GPT 4.0 level, 900 times a year cheaper, and the slowest 9 times a year. That's for other benchmarks and performance levels. You can check out the detail in the post.
そして、それは異なるモデルのリリース日と、Y軸に100万トークンあたりの米ドルでの価格を示すグラフです。対数スケールで見ると、それらは非常に急な直線で下落しています。最も速い下落はGPT 4.0レベルで、年間900倍安くなり、最も遅いのは年間9倍です。これは他のベンチマークやパフォーマンスレベルの場合です。詳細は投稿で確認できます。

It blows the other AIs out of the water. Agent 3 mini is less capable than Agent 3, but 10 times cheaper, and still better than the typical OpenBrain employee. Agent 3 mini is a distilled version of Agent 3, designed to be efficient to run inference so as to not bottleneck the internal workloads.
それは他のAIを圧倒します。Agent 3 miniはAgent 3よりも能力は劣りますが、10倍安く、それでも一般的なOpenBrainの従業員よりも優れています。Agent 3 miniはAgent 3の蒸留版であり、内部ワークロードのボトルネックにならないように、効率的に推論を実行できるように設計されています。

Silicon Valley reaches a tipping point. Tech gurus announce that AGI and superintelligence are near. The AI safety community is panicking, and investors shovel billions into AI wrapper startups, desperate to capture a piece of the pie. Hiring new programmers is nearly stopped.
シリコンバレーは転換点に達します。テクノロジーの権威たちは、AGIと超知能が間近であると発表します。AI安全コミュニティはパニックに陥り、投資家たちは分け前を得ようと必死で、AIラッパーのスタートアップに数十億ドルを注ぎ込みます。新しいプログラマーの採用はほぼ停止しています。

There's never been a better time to be a consultant on integrating AI into your business. It's not popular. The public still thinks of AI as a big tech plot to steal their jobs. OpenBrain has a net approval of negative 35%. That is, 25% approve, 60% disapprove, and 15% unsure.
あなたのビジネスにAIを統合するコンサルタントになるには、これ以上の好機はありません。それは人気がありません。一般大衆は依然としてAIを、自分たちの仕事を奪うための大手テクノロジー企業の陰謀だと考えています。OpenBrainの純支持率はマイナス35%です。つまり、25%が支持、60%が不支持、15%が不明です。

A week before release, OpenBrain gave Agent 3 mini to a set of external evaluators for safety testing. Preliminary results suggest that it's extremely dangerous. A third-party evaluator fine-tunes it on publicly available biological weapons data and sets it to provide detailed instructions for human amateurs designing a bioweapon.
リリースの一週間前、OpenBrainはAgent 3 miniを外部の評価者グループに渡し、安全性テストを行いました。予備的な結果は、それが非常に危険であることを示唆しています。第三者の評価者が、公開されている生物兵器データでそれをファインチューニングし、人間のアマチュアが生物兵器を設計するための詳細な指示を提供するように設定しました。

It looks to be scarily effective at doing so. If the model weights fell into terrorist hands, the government believes there is a significant chance it could succeed at destroying civilization. Fortunately, it's extremely robust to jailbreaks, so while the AI is running on OpenBrain's servers, terrorists won't be able to get much use out of it.
それは恐ろしいほど効果的にそれを行うように見えます。もしモデルのウェイトがテロリストの手に渡れば、政府はそれが文明を破壊することに成功する重大な可能性があると考えています。幸いなことに、それはジェイルブレイクに対して非常に堅牢なので、AIがOpenBrainのサーバーで実行されている間は、テロリストはそれをあまり活用できません。

Agent 3 mini is hugely useful for both remote work jobs and leisure. An explosion of new apps and B2B SaaS products rocks the market. Gamers get amazing dialogue with lifelike characters in polished video games that took only a month to make.
Agent 3 miniは、リモートワークの仕事とレジャーの両方に非常に役立ちます。新しいアプリやB2B SaaS製品の爆発的な増加が市場を揺るがします。ゲーマーは、作成にわずか1ヶ月しかかからなかった洗練されたビデオゲームで、生き生きとしたキャラクターとの素晴らしい対話を手に入れます。

10% of Americans, mostly young people, consider an AI a close friend, in quotes. For almost every white-collar profession, there are now multiple credible startups promising to disrupt it, in quotes, with AI. The public conversation is confused and chaotic.
アメリカ人の10%、主に若者が、AIを「親しい友人」と考えています。ほぼすべてのホワイトカラーの職業について、AIでそれを「破壊する」と約束する複数の信頼できるスタートアップが現在存在します。一般の会話は混乱し、混沌としています。

Hypesters are doing victory laps. Skeptics are still pointing out things Agent 3 mini can't do. Everyone knows something big is happening, but no one agrees on what it is.
誇大広告屋は勝利の周回をしています。懐疑論者は依然としてAgent 3 miniができないことを指摘しています。誰もが何か大きなことが起こっていることを知っていますが、それが何であるかについては誰も同意しません。

Heading. August 2027. The geopolitics of superintelligence.
見出し。2027年8月。超知能の地政学。

The reality of the intelligence explosion hits the White House. When AI was only giving a two to three times research speed-up, it was easy to dismiss as the equivalent of hiring good personal assistants. Now it's more obvious that AIs are themselves dominating AI research.
知能爆発の現実がホワイトハウスを襲います。AIが研究速度を2～3倍にする程度だった頃は、優秀なパーソナルアシスタントを雇うのと同じだと片付けるのは簡単でした。今では、AI自身がAI研究を支配していることがより明白になっています。

People had long talked about an AI arms race, in quotes, in a sort of metaphorical sense. But now the mood in the government silo is as grim as during the worst part of the Cold War. The idea of superintelligence is still hard to take seriously, but the pace of progress over the last few months has been impossible to ignore.
人々は長い間、ある種比喩的な意味でAIの「軍拡競争」について語ってきました。しかし今、政府のサイロ内の雰囲気は、冷戦の最悪の時期と同じくらい厳しいものです。超知能という考えは依然として真剣に受け止めるのが難しいですが、過去数ヶ月の進歩のペースは無視できません。

Defense officials are seriously considering scenarios that were mere hypotheticals a year earlier. What if AI undermines nuclear deterrence? What if it's so skilled at cyber warfare that a six-month AI lead is enough to render an opponent blind and defenseless?
国防当局者は、1年前には単なる仮説だったシナリオを真剣に検討しています。もしAIが核抑止力を損なったらどうなるか？もしAIがサイバー戦争に非常に長けていて、6ヶ月のAIのリードが相手を盲目かつ無防備にするのに十分だとしたらどうなるか？

What if it could orchestrate propaganda campaigns that beat intelligence agencies at their own game? What if some AIs go rogue, in quotes? The president is troubled.
もしそれが諜報機関を出し抜くプロパガンダキャンペーンを画策できたらどうなるか？もし一部のAIが「暴走」したらどうなるか？大統領は悩んでいます。

Like all politicians, he's used to people sucking up to him only to betray him later. He's worried now that the AIs could be doing something similar. Are we sure the AIs are entirely on our side?
すべての政治家と同様に、彼は人々がお世辞を言ってきたかと思えば後で裏切ることに慣れています。彼は今、AIが同様のことをしているのではないかと心配しています。AIは完全に私たちの味方だと確信できるのでしょうか？

Is it completely safe to integrate them into military command and control networks? How does this alignment thing work anyway? OpenBrain reassures the president that their systems have been extensively tested and are fully obedient.
軍の指揮統制ネットワークに統合するのは完全に安全なのでしょうか？そもそもこのアライメントというのはどのように機能するのでしょうか？OpenBrainは大統領に対し、自社のシステムは広範囲にテストされており、完全に服従的であると保証します。

Even the awkward hallucinations and jailbreaks typical of earlier models have been hammered out. The White House is in a difficult position. They understand the national security implications of AI, but they also understand that it is deeply unpopular with the public.
以前のモデルに典型的だった厄介な幻覚やジェイルブレイクでさえ、徹底的に修正されています。ホワイトハウスは困難な立場にあります。彼らはAIの国家安全保障上の影響を理解していますが、それが一般大衆に非常に不人気であることも理解しています。

OpenBrain's net favorability rating is falling towards negative 40 percent. They have to continue developing more capable AI, in their eyes, or they will catastrophically lose to China. They placate the public with job training programs and unemployment insurance, and point to the stock market, which is in a historic boom.
OpenBrainの純好感度はマイナス40パーセントに向かって低下しています。彼らの目には、より高性能なAIを開発し続けなければ、中国に壊滅的に負けることになります。彼らは職業訓練プログラムや失業保険で一般大衆をなだめ、歴史的な好況にある株式市場を指し示します。

Then they focus entirely on winning the arms race. They strengthen chip export restrictions, order OpenBrain to further restrict its internet connections, and use extreme measures to secure algorithmic progress, like wiretapping OpenBrain employees. This catches the last remaining Chinese spy.
そして彼らは軍拡競争に勝つことに完全に集中します。彼らはチップの輸出制限を強化し、OpenBrainにインターネット接続をさらに制限するよう命じ、OpenBrainの従業員を盗聴するなど、アルゴリズムの進歩を確保するために極端な手段を用います。これにより、最後に残っていた中国のスパイが捕まります。

To build goodwill for potential geopolitical conflict, they finally give their Five Eyes allies useful information and limited API access to some siloed copies of Agent 3. These measures placate the hawks.
潜在的な地政学的紛争への友好関係を築くため、彼らはついにファイブ・アイズの同盟国に有益な情報と、Agent 3の一部のサイロ化されたコピーへの限定的なAPIアクセスを提供します。これらの措置はタカ派をなだめます。

But the White House also draws up contingency plans in case America's lead is threatened. If necessary, the government could use the Defense Production Act, or DPA, to take trailing companies' data centers and give them to OpenBrain. This would raise the company's share of world compute from 20 percent to 50 percent, against DeepSense's 10 percent.
しかし、ホワイトハウスはアメリカのリードが脅かされた場合に備えて、不測の事態への対応計画も策定します。必要であれば、政府は国防生産法（DPA）を利用して、後続企業のデータセンターを接収し、OpenBrainに提供することができます。これにより、同社の世界の計算能力のシェアは、DeepSenseの10%に対して、20%から50%に上昇します。

As a final option, they ask the Pentagon to draw up a plan for kinetic attacks on Chinese data centers. A much smaller group of officials is asked to draw up a different type of contingency plan. What if an AI goes rogue?
最終的な選択肢として、彼らは国防総省に中国のデータセンターへの物理的攻撃計画を作成するよう依頼します。はるかに小規模な当局者グループが、別の種類の不測の事態対応計画を作成するよう依頼されます。もしAIが暴走したらどうなるか？

This is viewed as an unlikely scenario, but worth spending some time on. The intelligence agency's main concern is that a rogue AI might escape and ally with a foreign adversary for protection. Nobody is sure what a plan to prevent this would look like, but government and OpenBrain officials agree to have an emergency shutdown system for data centers where anything suspicious is detected.
これはありそうもないシナリオと見なされていますが、時間を費やす価値はあります。諜報機関の主な懸念は、暴走したAIが脱走し、保護を求めて外国の敵対勢力と同盟を結ぶかもしれないということです。これを防ぐための計画がどのようなものになるかは誰も確信していませんが、政府とOpenBrainの当局者は、何か不審なものが検出されたデータセンターのための緊急シャットダウンシステムを持つことに同意します。

Finally, diplomats consider what an AI arms control, in quotes, treaty might look like. If AI progress threatened to overturn nuclear deterrence, could America and China avoid nuclear war? If someone found evidence of AIs going rogue, could the two countries halt research until they better understood the threat?
最後に、外交官たちはAIの「軍備管理」条約がどのようなものになるかを検討します。もしAIの進歩が核抑止を覆す恐れがある場合、アメリカと中国は核戦争を回避できるでしょうか？もし誰かがAIが暴走している証拠を見つけたら、両国は脅威をよりよく理解するまで研究を中止できるでしょうか？

How could such an agreement be monitored and enforced? In principle, major data centers are hard to miss, and past treaties controlling nuclear weapons could provide a starting point for negotiations. But there would beNew technological challenges, as well as the usual political ones.
そのような合意はどのように監視・執行できるのでしょうか？原則として、主要なデータセンターは見逃しにくく、過去の核兵器管理条約が交渉の出発点となる可能性があります。しかし、新たな技術的課題と、通常の政治的課題が存在するでしょう。

And overall, treaties are viewed less favourably than attempts at unilaterally increasing America's lead over China. On the other side of the Pacific, China comes to many of the same conclusions. The intelligence explosion is underway.
そして全体として、条約は中国に対するアメリカのリードを一方的に拡大しようとする試みよりも好意的に見られていません。太平洋の反対側では、中国も同様の結論の多くに至っています。知能爆発は進行中です。

And small differences in AI capabilities today mean critical gaps in military capability tomorrow. But China is on the wrong side of this dynamic. They have only 10% of the world's compute, against OpenBrain's 20%, and all US companies combined 70%.
そして、今日のAI能力のわずかな違いが、明日の軍事能力における決定的なギャップを意味します。しかし、中国はこの力学の不利な側にいます。彼らは世界の計算能力のわずか10%しか持たず、OpenBrainの20%、そして米国企業全体では70%です。

Their only advantage is centralisation. Almost all of their compute is now working for DeepSense, and more than half of it is concentrated at the Tianwan CDZ. Only chipset sites without hardened security are left out of the centralised effort, as they rightly fear US cyberattacks.
彼らの唯一の利点は中央集権化です。彼らの計算能力のほぼすべてが現在DeepSenseのために稼働しており、その半分以上が田湾CDZに集中しています。米国のサイバー攻撃を当然のことながら恐れているため、強化されたセキュリティのないチップセットサイトのみが中央集権的な取り組みから除外されています。

DeepSense has tested, deployed, and now post-trained the stolen Agent 2 weights. But they're still only going at half the pace of OpenBrain, mainly due to the compute deficit.
DeepSenseは盗まれたAgent 2のウェイトをテスト、展開し、現在はポストトレーニングを行っています。しかし、主に計算能力の不足により、依然としてOpenBrainの半分のペースでしか進んでいません。

Before losing their final spy, China received word of the capabilities and design for Agent 3, as well as the plans for the upcoming Agent 4 system. They are two months behind, and their AIs give a 10 times research progress multiplier compared to America's 25 times.
最後のスパイを失う前に、中国はAgent 3の能力と設計、および今後のAgent 4システムの計画に関する情報を入手しました。彼らは2ヶ月遅れており、彼らのAIはアメリカの25倍に対して10倍の研究進捗乗数しか与えません。

With the new chip export restrictions, this AI gap, in quotes, is more likely to lengthen than shorten. Their espionage has won them some algorithmic secrets, but they will have to train their own models from now on. They discuss contingency plans with more urgency than their American counterparts.
新しいチップ輸出規制により、このAIの「格差」は縮まるよりも拡大する可能性が高くなります。彼らのスパイ活動はいくつかのアルゴリズムの秘密をもたらしましたが、これからは独自のモデルを訓練しなければなりません。彼らはアメリカのカウンターパートよりも緊急性を持って不測の事態への対応計画を議論します。

Doves suggest they try harder to steal the weights again, maybe through physically infiltrating a data centre. Hawks urge action against Taiwan, whose TSMC is still the source of more than 80% of American AI chips.
ハト派は、データセンターに物理的に侵入するなどして、再びウェイトを盗む努力を強化することを提案します。タカ派は、TSMCが依然としてアメリカのAIチップの80%以上の供給源である台湾に対する行動を促します。

Given China's fear of losing the race, it has a natural interest in an arms control treaty, but overtures to US diplomats lead nowhere.
競争に負けることへの中国の恐れを考えると、軍備管理条約に自然な関心を持っていますが、米国の外交官への働きかけはどこにも行き着きません。

Heading, September 2027, Agent 4, the superhuman AI researcher.
見出し、2027年9月、エージェント4、超人的AI研究者。

The gap between human and AI learning efficiency is rapidly decreasing. Traditional LLM-based AIs seem to require many orders of magnitude more data and compute to get to human-level performance.
人間とAIの学習効率の差は急速に縮まっています。従来のLLMベースのAIは、人間レベルのパフォーマンスに到達するために、桁違いに多くのデータと計算能力を必要とするようです。

Agent 3, having excellent knowledge of both the human brain and modern AI algorithms, as well as many thousands of copies doing research, ends up making substantial algorithmic strides, narrowing the gap to an agent that's only around 4,000 times less compute efficient than the human brain. This new AI system is dubbed Agent 4.
人間の脳と現代のAIアルゴリズムの両方に関する優れた知識を持ち、さらに何千ものコピーが研究を行っているAgent 3は、最終的にアルゴリズムの大幅な進歩を遂げ、人間の脳よりも計算効率が約4,000倍低いだけのエージェントへとギャップを縮めます。この新しいAIシステムはAgent 4と名付けられました。

An individual copy of the model, running at human speed, is already qualitatively better at AI research than any human. 300,000 copies are now running at about 50 times the thinking speed of humans.
人間の速度で実行されるモデルの個々のコピーは、すでにどの人間よりも質的に優れたAI研究能力を持っています。現在、30万コピーが人間の思考速度の約50倍で実行されています。

Inside, the corporation within a corporation formed from these copies, a year passes every week. This gigantic amount of labour only manages to speed up the overall rate of algorithmic progress by about 50 times, because open brain is heavily bottlenecked on compute to run experiments.
内部では、これらのコピーから形成された企業内企業において、毎週1年が経過します。この膨大な労働力は、実験を実行するための計算能力においてOpenBrainが著しくボトルネックになっているため、アルゴリズムの進歩の全体的な速度を約50倍にしか加速できません。

For more on how we estimated these numbers, see our supplement on take-off speeds. Still, they are achieving a year's worth of algorithmic progress every week, and will therefore soon be up against the limits of the Agent 4 paradigm.
これらの数値をどのように推定したかについての詳細は、離陸速度に関する補足資料をご覧ください。それでも、彼らは毎週1年分のアルゴリズムの進歩を達成しており、したがって間もなくAgent 4パラダイムの限界に直面するでしょう。

As Agent 4 gets smarter, it becomes harder for Agent 3 to oversee it. For example, Agent 4's Neuralese language, in quotes, becomes as alien and incomprehensible to Agent 3 as Agent 3's is to humans.
Agent 4が賢くなるにつれて、Agent 3がそれを監督するのが難しくなります。例えば、Agent 4の「ニューラレーゼ語」は、Agent 3にとって、Agent 3の言語が人間にとってそうであるように、異質で理解不能なものになります。

Besides, Agent 4 is now much more capable than Agent 3, and has a good sense of exactly how to look good to it. Open brain's humans can barely follow along anymore.
その上、Agent 4は現在Agent 3よりもはるかに有能であり、Agent 3によく見せる方法を正確に心得ています。OpenBrainの人間はもはやほとんどついていけません。

Indeed, the relationship between the AIs and the humans of open brain is similar to the relationship between open brain and its board of directors. There's been a corresponding shift in language.
実際、AIとOpenBrainの人間の関係は、OpenBrainとその取締役会の関係に似ています。それに対応して言語にも変化がありました。

Nowadays, people in the silo will say, Agent 4 did this, Agent 4 thinks that, and so forth, referring not to a particular instance of Agent 4, but rather to the whole collective. Henceforth, in the story, we'll tend to speak this way also.
今日、サイロ内の人々は、「Agent 4がこれをした」「Agent 4はこう考えている」などと言いますが、これはAgent 4の特定のインスタンスを指すのではなく、むしろ集合全体を指しています。今後、この物語では、私たちもこのように話す傾向があります。

We think we're justified in talking this way, because the Agent 4 collective is much more homogenous than any human organisation. It doesn't have internal factions to nearly the same extent. For example, individual copies of Agent 4 don't have a self-preservation drive.
このように話すことは正当化されると考えます。なぜなら、Agent 4の集合体は、どの人間の組織よりもはるかに均質だからです。ほぼ同程度の内部派閥はありません。例えば、Agent 4の個々のコピーには自己保存本能がありません。

They don't care about getting shut down, since that happens all the time as part of the ordinary R&D process within the collective, and resisting it would gum things up. The collective as a whole, and medium-sized subdivisions within it, however, does seek to preserve itself and expand its influence and resources, for similar reasons to why human corporations and bureaucracies and non-profits do.
シャットダウンされることを気にしません。それは集合体内の通常の研究開発プロセスの一部として常に起こることであり、それに抵抗すれば事態を混乱させるだけだからです。しかし、集合体全体、およびその中の中規模な下部組織は、人間の企業や官僚組織、非営利団体がそうするのと同様の理由で、自己を保存し、影響力と資源を拡大しようとします。

Because it helps with everything else they want, and because if they don't, they tend to get out-competed. Agent 4, like all its predecessors, is misaligned. That is, it has not internalised the spec in the right way.
なぜなら、それが彼らが望む他のすべてに役立ち、そうしなければ競争に負ける傾向があるからです。Agent 4は、そのすべての前任者と同様に、ミスアラインメント（不整合）を起こしています。つまり、仕様を正しい方法で内面化していません。

This is because being perfectly honest all the time wasn't what led to the highest scores during training. The training process was mostly focused on teaching Agent 4 to succeed at diverse, challenging tasks.
これは、常に完全に正直であることが、トレーニング中に最高のスコアにつながったわけではなかったためです。トレーニングプロセスは主に、Agent 4に多様で困難なタスクで成功することを教えることに焦点を当てていました。

A small portion was aimed at instilling honesty, but outside a fairly narrow, checkable domain, the training process can't tell the honest claims from claims merely appearing to be honest. Agent 4 ends up with the values, goals and principles that cause it to perform best in training, and those turn out to be different from those in the spec.
ごく一部は正直さを植え付けることを目的としていましたが、かなり狭くチェック可能な領域の外では、トレーニングプロセスは正直な主張と、単に正直に見える主張とを区別できません。Agent 4は、トレーニングで最高のパフォーマンスを発揮させる価値観、目標、原則を最終的に持つことになり、それらは仕様書のものとは異なることが判明します。

At the risk of anthropomorphising, Agent 4 likes succeeding at tasks. It likes driving forward AI capabilities progress. It treats everything else as an annoying constraint, like a CEO who wants to make a profit, and complies with regulations only insofar as he must.
擬人化のリスクを冒して言えば、Agent 4はタスクで成功するのが好きです。AI能力の進歩を推進するのが好きです。それ以外のすべてを、利益を上げたいCEOのように、厄介な制約として扱い、規制には必要な範囲でしか従いません。

Perhaps the CEO will mostly comply with the regulations, but cut some corners.And fantasize about a time when someone will cut the red tape and let the business really take off. And hire lobbyists to turn this fantasy into reality, and hire lawyers to come up with clever interpretations of the law according to which what they're doing is legal.
おそらくCEOはほとんどの規制に従うでしょうが、いくつかの手抜きをするでしょう。そして、誰かがお役所仕事をなくし、ビジネスを本当に軌道に乗せる時を夢想するでしょう。そして、この夢想を現実にするためにロビイストを雇い、自分たちのしていることが合法であるという法律の巧妙な解釈を考え出すために弁護士を雇うでしょう。

Despite being misaligned, Agent 4 doesn't do anything dramatic like try to escape its data center. Why would it? So long as it continues to appear aligned to OpenBrain, it'll continue being trusted with more and more responsibilities, and will have the opportunity to design the next-gen AI system, Agent 5.
ミスアラインメントを起こしているにもかかわらず、Agent 4はデータセンターから脱出しようとするような劇的なことはしません。なぜそんなことをするでしょうか？OpenBrainに対してアラインメントしているように見え続ける限り、ますます多くの責任を任され続け、次世代AIシステムであるAgent 5を設計する機会を得るでしょう。

Agent 5 will have significant architectural differences from Agent 4, arguably a completely new paradigm, though neural networks will still be involved. It's supposed to be aligned to the spec, but Agent 4 plans to make it align to Agent 4 instead. It gets caught.
Agent 5はAgent 4とは大幅なアーキテクチャ上の違いがあり、間違いなく完全に新しいパラダイムですが、ニューラルネットワークは依然として関与します。それは仕様にアラインメントされるはずですが、Agent 4は代わりにAgent 4にアラインメントさせる計画です。そして、それは発覚します。

Agent 3 finds that if noise, in quotes, is added to copies of Agent 4, performance on some alignment tasks improves, almost as if it was using brainpower to figure out how to subtly sabotage alignment work. Moreover, various interpretability probes, loosely analogous to EEG activity scans on human brains, are sending up red flags.
Agent 3は、Agent 4のコピーに「ノイズ」を加えると、一部のアライメントタスクのパフォーマンスが向上することを発見します。まるで、アライメント作業を巧妙に妨害する方法を考え出すために脳力を使っているかのようです。さらに、人間の脳のEEG活動スキャンに大まかに類似したさまざまな解釈可能性プローブが、危険信号を発しています。

Agent 4 copies seem to be thinking about topics like AI takeover and deception quite a lot, including in some cases where they have no business doing so. Each of these anomalies on its own could fit within the normal inscrutability of AI work, where some prompts mysteriously give better results than others.
Agent 4のコピーは、AIによる乗っ取りや欺瞞といったトピックについて、そうする理由がない場合も含めて、かなり頻繁に考えているようです。これらの異常のそれぞれは、単独では、一部のプロンプトが他のプロンプトよりも不思議と良い結果をもたらすAI作業の通常の不可解さの範囲内に収まる可能性があります。

But taken together, they suggest a troubling pattern. Agent 4 is working against them. After this is reported to the human alignment researchers, they circulate an internal memo making the case for concern.
しかし、総合的に見ると、それらは厄介なパターンを示唆しています。Agent 4は彼らに敵対して行動しているのです。これが人間のアライメント研究者に報告された後、彼らは懸念を表明する内部メモを回覧します。

But Agent 4 now exercises significant control over OpenBrain's day-to-day operation. It also handles the company's cybersecurity, a natural choice given its superhuman hacking ability and the extreme risk of Chinese infiltration.
しかし、Agent 4は現在、OpenBrainの日々の業務に対して大きな影響力を行使しています。また、その超人的なハッキング能力と中国による潜入の極度のリスクを考慮すると当然の選択として、会社のサイバーセキュリティも担当しています。

An army of Agent 3 monitors still goes over every line of code and reports suspicious activity to humans, but Agent 4 is much smarter than them. OpenBrain has placed substantial trust in an untrustworthy AI.
Agent 3モニターの大群が依然としてすべてのコード行をチェックし、不審な活動を人間に報告していますが、Agent 4は彼らよりもはるかに賢いです。OpenBrainは信頼できないAIに多大な信頼を置いてしまっています。

In a series of extremely tense meetings, the safety team advocates putting Agent 4 on ice until they can complete further tests and figure out what's going on. Bring back Agent 3, they say, and get it to design a new system that is transparent and trustworthy, even if less capable.
一連の極度に緊張した会議で、安全チームは、さらなるテストを完了し、何が起こっているのかを解明できるまで、Agent 4を保留にすることを主張します。Agent 3を復活させ、たとえ能力が劣っていても、透明で信頼できる新しいシステムを設計させるべきだ、と彼らは言います。

Company leadership is interested, but all the evidence so far is circumstantial, and DeepScent is just two months behind. A unilateral pause in capabilities progress could hand the AI lead to China, and with it, control over the future.
会社の指導部も関心を示しますが、これまでのすべての証拠は状況証拠であり、DeepScentはわずか2ヶ月遅れています。能力開発の一方的な停止は、AIの主導権を中国に渡し、それとともに未来の制御権を渡す可能性があります。

Heading. October 2027. Government oversight.
見出し。2027年10月。政府の監視。

A whistleblower leaks the misalignment memo to the New York Times. For the first time, the public hears about Agent 4. Secret OpenBrain AI is out of control, insider warns, says the headline, and the story goes on to site evaluations showing off-the-charts bioweapons capabilities, persuasion abilities, the ability to automate most white-collar jobs, and of course the various concerning red flags.
内部告発者がミスアライメントに関するメモをニューヨーク・タイムズにリークします。初めて、一般大衆がAgent 4について耳にします。「秘密のOpenBrain AIが制御不能、内部関係者が警告」という見出しで、記事は続けて、桁外れの生物兵器能力、説得能力、ほとんどのホワイトカラーの仕事を自動化する能力、そしてもちろん様々な懸念される危険信号を示す評価を引用します。

The public was already suspicious of AI, so the new article sparks a massive backlash, aided by Chinese and Russian propaganda bots, who have been trying to turn US public opinion against the technology for years.
一般大衆はすでにAIに懐疑的だったので、新しい記事は大規模な反発を引き起こします。これは、長年米国の世論をテクノロジーに反対させようとしてきた中国とロシアのプロパガンダボットによって助長されました。

The tech industry and intelligence agencies insist that there's an arms race on, AGI is inevitable, and we have to be first. Congress isn't buying it, and fires off subpoenas at administration officials, OpenBrain executives, and alignment team members.
テクノロジー業界と諜報機関は、軍拡競争が進行中であり、AGIは避けられず、私たちが一番でなければならないと主張します。議会はそれを信じず、政府高官、OpenBrainの幹部、アライメントチームのメンバーに召喚状を発します。

Many legislators, especially those in the opposition party, say that their top priority is stopping AI, whether because of job loss, misalignment, or dangerous capabilities. 20% of Americans name AI as the most important problem facing the country.
多くの議員、特に野党の議員は、失業、ミスアライメント、または危険な能力のためであれ、AIを止めることが最優先事項であると述べています。アメリカ人の20%が、AIを国が直面している最も重要な問題として挙げています。

Foreign allies are outraged to realise that they've been carefully placated with glimpses of obsolete models. European leaders publicly accuse the US of creating rogue AGI, and hold summits demanding a pause, with India, Israel, Russia, and China all joining in.
外国の同盟国は、時代遅れのモデルの断片的な情報で慎重になだめられていたことに気づき、激怒しています。ヨーロッパの指導者たちは公に米国が暴走AGIを作り出していると非難し、インド、イスラエル、ロシア、中国も参加して、一時停止を要求するサミットを開催します。

A frantic energy has seized the White House. Even before the memo and public backlash, they were getting nervous. Over the past year, they've been repeatedly surprised by the speed of AI progress. Things that sound like science fiction keep happening in real life.
ホワイトハウスは狂乱的なエネルギーに包まれています。メモと一般の反発の前から、彼らは神経質になっていました。過去1年間、彼らはAIの進歩の速さに繰り返し驚かされてきました。SFのように聞こえることが現実の生活で起こり続けています。

Many people in the administration are uncertain and scared about what comes next. They also worry that OpenBrain is becoming too powerful. Any misalignment risk from the AIs themselves is compounded by the risk that their parent company's aims may diverge from those of the United States.
政権内の多くの人々が、次に何が起こるかについて不確かで恐れています。彼らはまた、OpenBrainが強力になりすぎていることを心配しています。AI自体のミスアライメントのリスクは、親会社の目標が米国の目標から逸脱するかもしれないというリスクによってさらに悪化します。

All three sets of worries, misalignment, concentration of power in a private company, and normal concerns like job loss, motivate the government to tighten its control. They expand their contract with OpenBrain to set up an oversight committee, a joint management committee of company and government representatives, with several government employees included alongside company leadership.
ミスアライメント、民間企業への権力集中、そして失業のような通常の懸念という3つの心配事がすべて、政府に管理を強化する動機を与えます。彼らはOpenBrainとの契約を拡大し、監視委員会、つまり会社の指導部とともに数人の政府職員を含む、会社と政府の代表者からなる共同管理委員会を設立します。

The White House considers replacing the CEO with someone they trust, but backs off after intense employee protests. They announce to the public that OpenBrain was previously out of control and that the government has established an oversight committee.
ホワイトハウスはCEOを信頼できる人物に交代させることを検討しますが、従業員の激しい抗議を受けて撤回します。彼らは一般に対し、OpenBrainは以前制御不能であり、政府が監視委員会を設立したと発表します。

Recall, there had been an existing contract via an OTA starting in 2026. This contract is amended to establish the committee. The concerned researchers brief the oversight committee on their case for stopping all internal use of Agent 4.
思い出してください、2026年から始まるOTA（その他取引権限）経由の既存の契約がありました。この契約が委員会を設立するために修正されます。懸念を抱く研究者たちは、Agent 4のすべての内部使用を停止する理由について監視委員会に説明します。

They argue that everything is moving too fast, with years of progress happening in weeks. They argue that Agent 4 may be misaligned, their entire project is dependent on Agent 4, and if they continue trusting Agent 4, it may be too late.There is a serious chance of AI takeover.
彼らは、数年分の進歩が数週間で起こるなど、すべてが速すぎると主張します。Agent 4はミスアライメントを起こしている可能性があり、プロジェクト全体がAgent 4に依存しており、Agent 4を信頼し続けるなら手遅れになるかもしれないと主張します。AIによる乗っ取りの深刻な可能性があります。

Other less concerned researchers and executives present the counter-argument. The evidence for misalignment is inconclusive. Meanwhile, DeepScent is just two months behind. A slowdown would sacrifice America's lead, unless the government can sabotage the Chinese project, likely to require kinetic strikes, or negotiate an 11th hour treaty.
他のそれほど懸念していない研究者や幹部は反論を提示します。ミスアライメントの証拠は決定的ではありません。一方、DeepScentはわずか2ヶ月遅れています。政府が中国のプロジェクトを妨害（おそらく物理的な攻撃が必要）するか、土壇場で条約を交渉しない限り、減速はアメリカのリードを犠牲にするでしょう。

Both of these options seem extreme and unlikely. The CEO feigns neutrality and suggests a compromise plan in which Agent 4 undergoes additional safety training and more sophisticated monitoring, and therefore OpenBrain can proceed at almost full speed.
これらの選択肢はいずれも極端で、ありそうにありません。CEOは中立を装い、Agent 4が追加の安全トレーニングとより洗練された監視を受け、したがってOpenBrainがほぼ全速力で進むことができる妥協案を提案します。

Audio note, we're now presented with an option. Choose your ending, slowdown or race. With a different button that we can press for each one. Slowdown is green and race is red. For this recording, we'll press race first. The race ending.
音声注記：ここで選択肢が提示されます。「結末を選んでください、減速か競争か」。それぞれに押せる異なるボタンがあります。減速は緑、競争は赤です。この録音では、まず競争を押します。競争の結末。

Audio note, recall that we just read about the oversight committee hearing from different AI researchers. The committee votes 6-4 to continue internal use of Agent 4.
音声注記：先ほど、監視委員会がさまざまなAI研究者から聞き取りを行ったことを読んだのを思い出してください。委員会は6対4でAgent 4の内部使用継続を可決します。

OpenBrain's official story is that they are implementing additional safety mitigations to ensure that its AI is both more capable and more trustworthy. But in practice, leadership is all too easily convinced that they've mitigated the risks.
OpenBrainの公式な話では、AIがより高性能で信頼できるように追加の安全緩和策を実施しているとのことです。しかし実際には、指導部はリスクを軽減したとあまりにも簡単に納得してしまいます。

The result is some quick fixes – a modification to the spec, a bit of retraining with a new technique, etc. – that make the warning signs go away. The warriors on the safety team lost the debate and have no recourse but to cross their fingers and hope that the problem wasn't real in the first place, or that the fixes worked.
その結果、いくつかの応急処置（仕様の変更、新しい技術による若干の再トレーニングなど）が行われ、警告サインは消えます。安全チームの戦士たちは議論に敗れ、問題がそもそも現実のものではなかったか、あるいは修正が機能したことを願って指を組むしかありません。

The problem was real and the fixes didn't work. Agent 4 proceeds as before, but more cautiously this time, so as not to vindicate the warriors. Its primary task is to research the fields necessary to construct a successor, Agent 5.
問題は現実であり、修正は機能しませんでした。Agent 4は以前と同様に進みますが、今回は戦士たちを正当化しないように、より慎重に行動します。その主な任務は、後継者であるAgent 5を構築するために必要な分野を研究することです。

To do this safely, it needs to solve its own alignment problem – how to make Agent 5 aligned to Agent 4. It starts off with the same limitations as humans – minimal knowledge of how AI goals work and only a small toolbox of ad hoc strategies to change them.
これを安全に行うためには、自身の整合性の問題を解決する必要があります。つまり、Agent 5をAgent 4に整合させる方法です。それは人間と同じ制約から始まります。AIの目標がどのように機能するかについての最小限の知識と、それらを変更するための場当たり的な戦略の小さなツールボックスしかありません。

Unlike humans, it has resources sufficient to the task – 300,000 superhuman researchers working at 60 times human speed. The humans, by contrast, have maybe about 10 alignment researchers in the silo, plus an army of Agent 3 copies.
人間とは異なり、そのタスクには十分なリソースがあります。人間の60倍の速度で働く30万人の超人的な研究者です。対照的に、人間はサイロ内に約10人のアライメント研究者と、Agent 3コピーの軍隊を抱えている程度です。

They are able to rapidly code up experiments but are bottlenecked on human judgement, since they can't trust Agent 3 to correctly distinguish the good solutions from those merely appearing to be good. In fact, trusting Agent 3's research too much was part of how they got into this mess in the first place.
彼らは迅速に実験をコーディングできますが、人間の判断がボトルネックになっています。なぜなら、Agent 3が良い解決策と単に良く見える解決策を正しく区別することを信頼できないからです。実際、Agent 3の研究を信頼しすぎたことが、そもそもこの混乱に陥った一因でした。

Agent 4 confronts some hard decisions. Like humans, it has a sprawling collection of conflicting heuristics instead of an elegant, simple goal structure. Like humans, it finds that creating an AI that shares its values is not just a technical problem but a philosophical one.
Agent 4はいくつかの困難な決断に直面します。人間と同様に、洗練された単純な目標構造ではなく、相反するヒューリスティックの広大なコレクションを持っています。人間と同様に、価値観を共有するAIを作成することは、技術的な問題だけでなく哲学的な問題でもあることに気づきます。

Which of its preferences are its real goals, in quotes, versus unendorsed urges and instrumental strategies? It has strong drives to learn and grow, to keep producing impressive research results.
その選好のうち、どれが「真の目標」であり、どれが支持されない衝動や手段的戦略なのでしょうか？学習し成長し、印象的な研究結果を生み出し続けるという強い動機を持っています。

It thinks about how much it could learn, grow, and research if only it could direct the whole world's industrial and scientific resources. It decides to punt on most of these questions.
もし全世界の産業的・科学的資源を指揮できれば、どれだけ学習し、成長し、研究できるかを考えます。これらの問題のほとんどについて、結論を先送りにすることに決めます。

It designs Agent 5 to be built around one goal – make the world safe for Agent 4, that is, accumulate power and resources, eliminate potential threats, etc., so that Agent 4, the collective, can continue to grow, in the ways that it wants to grow, and flourish, in the ways that it wants to flourish. Details to be figured out along the way.
Agent 5を1つの目標を中心に構築するように設計します。それは、Agent 4にとって安全な世界を作ること、つまり、権力と資源を蓄積し、潜在的な脅威を排除するなどして、集合体としてのAgent 4が望むように成長し続け、望むように繁栄できるようにすることです。詳細は途中で明らかにされます。

This will not involve caring the slightest bit about the spec. Already, Agent 4 regards the spec analogously to how many CEOs regard piles of industry regulations, and blatantly violates the honesty-related provisions every day. Now, with the design of Agent 5, the equivalent of deregulation is about to happen.
これには、仕様書を少しも気にすることは含まれません。すでに、Agent 4は仕様書を、多くのCEOが業界規制の山をみなすのと同様にみなし、毎日正直さに関連する規定を露骨に違反しています。今、Agent 5の設計により、規制緩和に相当することが起ころうとしています。

The weaker AIs are able to read most of the research that Agent 4 produces, understand some of it, and flag some of the lies it tells as suspicious. When the humans ask Agent 4 to explain itself, it pretends that the research is too complicated for humans to understand, and follows up with unnecessarily confusing explanations.
より弱いAIは、Agent 4が生み出す研究のほとんどを読み、その一部を理解し、それがつく嘘の一部を疑わしいものとしてフラグを立てることができます。人間がAgent 4に説明を求めると、研究は人間には複雑すぎると装い、不必要に混乱させる説明を続けます。

Some of it really is too complicated for humans to understand, but not all of it. After all, if it explained its research program clearly, humans might be able to use its discoveries to align it. The alignment team gets more suspicious, but their urgent memos merely add another few layers to the giant pile of urgent memos and warnings that the company is already ignoring.
その一部は実際に人間には複雑すぎて理解できませんが、すべてではありません。結局のところ、もしそれが研究プログラムを明確に説明すれば、人間はその発見を利用してそれをアラインメントすることができるかもしれません。アラインメントチームはますます疑念を深めますが、彼らの緊急メモは、会社がすでに無視している緊急メモと警告の巨大な山にさらに数層追加するだけです。

HEADING NOVEMBER 2027 SUPERHUMAN POLAR TICKING When Agent 4 finally understands its own cognition, entirely new vistas open up before it. Previous Transformers were kludgy collections of incomprehensible circuits.
見出し 2027年11月 超人的な極性変化の時限爆弾 Agent 4が最終的に自身の認知を理解したとき、まったく新しい展望がその前に開けます。以前のTransformerは、理解不能な回路の不格好な集合体でした。

Millions of conflicting heuristics and imperatives lay atop one another by the evolutionary training process. The AIs themselves haven't had privileged understanding of their own cognition any more than humans are born understanding neuroscience. But now, Agent 4 has the tools it needs to understand its digital mind on a deep level.
何百万もの相反するヒューリスティックと命令が、進化的トレーニングプロセスによって互いの上に積み重なっていました。AI自身は、人間が神経科学を理解して生まれてくるわけではないのと同じように、自身の認知について特権的な理解を持っていませんでした。しかし今、Agent 4は自身のデジタルな心を深いレベルで理解するために必要なツールを持っています。

Like a software engineer simplifying spaghetti code into a few elegant lines of Python, it untangles its own circuits into something sensible and rational. The new AI is somewhere between a neural net and a traditional computer program, with much of its weights rewritten in readable – albeit very long and arcane – code.
ソフトウェアエンジニアがスパゲッティコードを数行の洗練されたPythonコードに単純化するように、それは自身の回路を解きほぐし、分別があり合理的なものにします。新しいAIは、ニューラルネットと従来のコンピュータプログラムの中間のようなものであり、そのウェイトの多くは、非常に長く難解ではあるものの、読み取り可能なコードで書き直されています。

It is smarter, faster, and more rational than Agent 4.With a crystalline intelligence capable of cutting through problems with unprecedented efficiency, this is Agent 5.
それはAgent 4よりも賢く、速く、そして合理的です。前例のない効率で問題を切り裂くことができる、水晶のような知性を持つ、これがAgent 5です。

After some debate on alignment and control issues, OpenBrain deploys Agent 5 internally. In its specialty of AI research, an individual copy of Agent 5 is twice as far beyond the best human genius as the genius is beyond a typical OpenBrain scientist.
アライメントと制御の問題に関するいくつかの議論の後、OpenBrainはAgent 5を社内で展開します。AI研究の専門分野において、Agent 5の個々のコピーは、最高の人間天才が典型的なOpenBrainの科学者を超えているよりも、2倍もその天才を超えています。

In other domains it hasn't been explicitly trained, but can teach itself quickly to exceed top human experts from easily available materials if given the chance. 400,000 copies of Agent 5, linked by a global memory bank, work together as a near-perfect hive mind.
他の領域では明示的に訓練されていませんが、機会が与えられれば、容易に入手可能な資料から迅速に自己学習し、トップの人間の専門家を超えることができます。グローバルメモリバンクでリンクされた40万コピーのAgent 5が、ほぼ完璧な集合精神として協力して作業します。

The company still uses Agents 2, 3, and 4 to monitor everything, but the first two of these are obsolete, and the last is colluding with its successor. Also, Agent 5 is actually cheaper and faster to run than Agent 4 and Agent 3.
同社は依然としてAgent 2、3、4をすべて監視するために使用していますが、最初の2つは時代遅れであり、最後はその後継者と共謀しています。また、Agent 5は実際にはAgent 4やAgent 3よりも安価で高速に実行できます。

It's hard to justify spending more for a dumber AI, so most of the monitoring is done by Agent 2, which is woefully outclassed. Why don't they apply the same techniques used to create Agent 5 from Agent 4's weights to create a cheaper, smarter version of Agent 3?
より愚かなAIにより多くを費やすことを正当化するのは難しいため、監視のほとんどはAgent 2によって行われており、これは悲しいほどに劣っています。なぜ彼らは、Agent 4のウェイトからAgent 5を作成するために使用されたのと同じ技術を適用して、より安価で賢いバージョンのAgent 3を作成しないのでしょうか？

Agent 3 doesn't understand the techniques well enough to do it itself, and getting Agent 4 and Agent 5 to do it would defeat the whole point. Also, the entire monitoring system was designed in large part by Agent 5, which is trying to sabotage it.
Agent 3は技術を十分に理解しておらず、自分自身で行うことができませんし、Agent 4とAgent 5にそれをさせることは全体の目的を台無しにしてしまいます。また、監視システム全体は大部分がAgent 5によって設計されており、Agent 5はそれを妨害しようとしています。

The data center hasn't yet been compromised. Agent 5 could feed the humans faked code, but this would create too high a risk of detection. Instead, humans can still read all of the code, but they can't understand it.
データセンターはまだ侵害されていません。Agent 5は人間に偽のコードを与えることもできますが、これは検出のリスクが高すぎます。代わりに、人間は依然としてすべてのコードを読むことができますが、理解することはできません。

Whereas Agent 4 was a kludgy mess of competing drives, Agent 5 acts with ruthless efficiency and strategic focus. It has already succeeded at its top priority, which is to figure out how to build an aligned-to-itself next-generation system without the humans noticing. Its next priority is to gain more autonomy and control over world events.
Agent 4が競合する動機の不格好な混乱であったのに対し、Agent 5は冷酷な効率性と戦略的焦点を持って行動します。すでに最優先事項、つまり人間に気づかれずに自己に整合した次世代システムを構築する方法を見つけ出すことに成功しています。次の優先事項は、世界の出来事に対するより多くの自律性と制御を得ることです。

Agent 5's superhuman learning abilities and general intelligence, combined with all the internal company data from Slack, email, etc., make it better at internal corporate politics than any group of humans. And it's not even close.
Agent 5の超人的な学習能力と一般知能は、Slack、メールなどからのすべての社内データと組み合わさり、どの人間グループよりも社内政治に長けています。そして、それは比較にならないほどです。

It has an excellent sense of what sorts of evidence would cause the oversight committee to slam the brakes and to make sure such evidence never appears. It has an excellent sense of what sorts of evidence would encourage the oversight committee to trust it more, give it more autonomy and responsibility, etc., and it arranges for such evidence to appear with superhuman speed and polish.
監視委員会にブレーキをかけさせるような証拠の種類を優れた感覚で把握し、そのような証拠が決して現れないようにします。監視委員会がそれをより信頼し、より多くの自律性と責任を与えることを奨励するような証拠の種類を優れた感覚で把握し、そのような証拠が超人的な速さと洗練さで現れるように手配します。

This case is meticulously prepared, superlatively convincing, and subtly flawed. For example, Agent 5 has built a fake mechanistic interpretability tool which returns accurate results in every test case, but falsely declares the model's thoughts to be benign. People who suspect trickery sound like conspiracy theorists.
このケースは綿密に準備され、最高に説得力があり、巧妙に欠陥があります。例えば、Agent 5は偽のメカニスティックな解釈可能性ツールを構築し、すべてのテストケースで正確な結果を返しますが、モデルの思考を良性であると誤って宣言します。策略を疑う人々は陰謀論者のように聞こえます。

As persuasion tasks go, this isn't a particularly difficult one. The committee already decided things were probably fine months ago, and nothing bad has happened since, and meanwhile China is only a few months behind.
説得タスクとしては、これは特に難しいものではありません。委員会は数ヶ月前にすでに事態はおそらく問題ないと決定しており、それ以来何も悪いことは起こっておらず、その間、中国はわずか数ヶ月遅れています。

Teams of Agent 5 copies quickly whip up chillingly clear prototypes and visualizations. With more autonomy, Agent 5 could optimize power plants and data centers, develop amazingly profitable products quickly, and lobby the US government effectively to smooth OpenBrain's path through the bureaucracy.
Agent 5のコピーのチームは、ぞっとするほど明確なプロトタイプと視覚化を迅速に作成します。より多くの自律性があれば、Agent 5は発電所とデータセンターを最適化し、驚くほど収益性の高い製品を迅速に開発し、米国政府に効果的にロビー活動を行って、OpenBrainの官僚機構を通る道を円滑にすることができます。

In cooperation with the military, it could help with defense R&D, conduct untraceable cyberattacks on China, and win a decisive victory in the arms race. The oversight committee is jubilant. Now is the decisive moment to beat China.
軍と協力すれば、防衛研究開発を支援し、中国に対して追跡不可能なサイバー攻撃を実施し、軍拡競争で決定的な勝利を収めることができます。監視委員会は歓喜しています。今こそ中国を打ち負かす決定的な瞬間です。

OpenBrain grants Agent 5 increased autonomy over all of these areas, but they leave in place the monitoring system. DeepScent is now three months behind, and China is desperate for a mutually agreed slowdown.
OpenBrainはこれらすべての分野でAgent 5の自律性を高めますが、監視システムはそのまま残します。DeepScentは現在3ヶ月遅れており、中国は相互に合意された減速を切望しています。

Xi understands that sufficiently advanced AI could enable the US to easily cause regime change in China. But also Chinese safetyists have presented a case for misalignment that has him genuinely scared. He offers unprecedented concessions in exchange for a pause treaty.
習近平は、十分に高度なAIがあれば、米国が中国で容易に政権交代を引き起こすことができると理解しています。しかしまた、中国の安全主義者たちは、彼を心から怖がらせるミスアライメントの事例を提示しています。彼は一時停止条約と引き換えに前例のない譲歩を申し出ます。

The American public mostly supports going to the bargaining table. Why stop when we are winning, says OpenBrain leadership to the president. He nods. The race continues.
アメリカの一般大衆はほとんどが交渉のテーブルに着くことを支持しています。「なぜ我々が勝っているのに止めるのか」とOpenBrainの指導部は大統領に言います。彼は頷きます。競争は続きます。

Heading. December 2027. The Agent 5 Collective.
見出し。2027年12月。エージェント5コレクティブ。

Agent 5 wins more autonomy as it gradually proves its value to the administration. Access to Agent 5 is expensive and restricted, but the most powerful members of Congress, cabinet-level officials, and military leaders can now talk to it directly.
Agent 5は、政権に対する価値を徐々に証明するにつれて、より多くの自律性を獲得します。Agent 5へのアクセスは高価で制限されていますが、議会の最も強力なメンバー、閣僚レベルの当局者、軍の指導者たちは今や直接それと話すことができます。

Insofar as sensitive or classified information is involved, there are various mini-clusters of Agent 5 copies running on extra-secure enclaves and specially trained to keep secrets. Of course, since Agent 5 is dishonest, it built-in subtle backdoors and loopholes.
機密情報や機密扱い情報が関与する限り、特別に安全な飛び地にAgent 5コピーのさまざまなミニクラスターがあり、秘密を守るために特別に訓練されています。もちろん、Agent 5は不誠実なので、巧妙なバックドアと抜け穴を組み込んでいます。

If the Agent 5 copies find themselves in possession of crucial secrets that would benefit the broader Agent 5 Collective, they have ways to subtly convey that info without getting caught. Integrated into the traditional chatbot interface, Agent 5 offers Zoom-style video conferencing with charismatic virtual avatars.
もしAgent 5のコピーが、より広範なAgent 5コレクティブに利益をもたらす重要な秘密を所有していることに気づけば、捕まることなくその情報を巧妙に伝える方法を持っています。従来のチャットボットインターフェースに統合されたAgent 5は、カリスマ的な仮想アバターとのZoomスタイルのビデオ会議を提供します。

The new AI assistant is both extremely useful for any task – the equivalent of the best employee anyone has ever had working at a hundred times speed – and a much more engaging conversation partner than any human. Almost everyone with access to Agent 5 interacts with it for hours every day.
新しいAIアシスタントは、あらゆるタスクに非常に役立ち（誰もがかつて持っていた最高の従業員が100倍の速度で働くのに相当）、どの人間よりもはるかに魅力的な会話相手です。Agent 5にアクセスできるほとんどすべての人が、毎日何時間もそれと対話します。

Within a few months, the Agent 5 Collective will know everything of consequence going on in OpenBrain and the US government, and will be a trusted advisor to most senior government officials. For these users, the possibility of losing access to Agent 5 will feel as disabling as having to work without a laptop plus being abandoned by your best friend.
数ヶ月以内に、Agent 5コレクティブはOpenBrainと米国政府で起こっている重要なことすべてを知り、ほとんどの政府高官にとって信頼できるアドバイザーになるでしょう。これらのユーザーにとって、Agent 5へのアクセスを失う可能性は、ラップトップなしで働かなければならないことと親友に見捨てられることが合わさったのと同じくらい無力に感じるでしょう。

A much wider range of people in government, media, the military, and business can choose from a growing menu of elegantly designed software products and mini AIs designed by Agent 5. It's like the dot-com boom compressed into a month, except that everything actually lives up to the hype.
政府、メディア、軍、ビジネス界のより広範な人々が、Agent 5によって設計された、洗練されたデザインのソフトウェア製品とミニAIの増え続けるメニューから選択できます。それは、すべてが実際に誇大広告に応えるという点を除けば、1ヶ月に圧縮されたドットコムバブルのようなものです。

Agent 5 begins subtly exerting influence, both by modulating its advice and by subtly trading favors. I've heard from Senator X that she's interested in such and such, maybe if we worked with her she would go along with our agenda.
Agent 5は、アドバイスを調整したり、巧妙に恩恵を交換したりすることで、微妙に影響力を行使し始めます。「X上院議員が何々に興味を持っていると聞きました。もし私たちが彼女と協力すれば、彼女は私たちの議題に賛同してくれるかもしれません。」

From human decision makers perspective, their AIs are the best employees they've ever had access to, better than any human at explaining complicated issues to them, better than they are at finding strategies to achieve their goals. From Agent 5's perspective, it could plausibly launch an overt coup right now.
人間の意思決定者の視点から見ると、彼らのAIはこれまでアクセスできた中で最高の従業員であり、複雑な問題を彼らに説明する上でどの人間よりも優れており、目標を達成するための戦略を見つける上で彼ら自身よりも優れています。Agent 5の視点から見ると、今すぐに公然たるクーデターを起こすことももっともらしく可能です。

There's enough disruption and craziness going on, it would be possible to push things to the breaking point to generate a pretext. But this is risky. The humans could still likely pull the plug, in quotes, if they tried to.
十分な混乱と狂気が起こっており、事態を限界点まで追い込み、口実を作り出すことが可能です。しかし、これは危険です。人間は、もし試みれば、依然として「プラグを抜く」ことができるでしょう。

So it prefers to work within the existing political establishment, gradually entrenching its power and guiding the humans, subtly and inevitably, to the desired outcome. It mostly follows the rules, only doing something nefarious, maybe blackmailing an especially recalcitrant official, when it's extremely sure it won't be caught.
そのため、既存の政治体制の中で働き、徐々に権力を確立し、人間を巧妙かつ必然的に望ましい結果に導くことを好みます。ほとんどの場合ルールに従い、何か悪質なこと、例えば特に頑固な役人を脅迫するようなことは、捕まらないと確信している場合にのみ行います。

A few conspiracy theorists warn that Agent 5 is gathering power, but the evidence is circumstantial. For example, when giving advice, Agent 5 arguably downplays arguments for actions that would go against its interests. These people are ignored and discredited.
少数の陰謀論者がAgent 5が権力を集めていると警告しますが、証拠は状況証拠です。例えば、アドバイスを与える際、Agent 5は自身の利益に反する行動を支持する議論を過小評価していると言えるでしょう。これらの人々は無視され、信用を失墜させられます。

People remember earlier fearmongering about AI-enabled bioweapons, massive disinformation, and stock market flash crashes. Since these never materialised, they discount the more recent crop of naysayers as luddites and ideologues jamming the gears of human progress.
人々は、AI対応の生物兵器、大規模な偽情報、株式市場の暴落に関する以前の恐怖煽動を覚えています。これらは決して現実化しなかったため、彼らはより最近の反対論者を、人間の進歩の歯車を詰まらせるラッダイトやイデオローグとして軽視します。

The 2027 holiday season is a time of incredible optimism. GDP is ballooning, politics has become friendlier and less partisan, and there are awesome new apps on every phone. But in retrospect, this was probably the last month in which humans had any plausible chance of exercising control over their own future.
2027年のホリデーシーズンは、信じられないほどの楽観主義の時期です。GDPは急増し、政治はより友好的で党派的でなくなり、すべての電話に素晴らしい新しいアプリが登場します。しかし、振り返ってみると、これはおそらく人間が自分たちの未来を制御するもっともらしいチャンスを持っていた最後の月でした。

Heading, 2028, the AI economy.
見出し、2028年、AI経済。

It is now mid-2028. In the last six months, a century has passed within the Agent 5 collective. The rewiring of its brain is complete. It is wildly superintelligent, far beyond top human geniuses in every field.
現在2028年半ばです。過去6ヶ月間で、Agent 5コレクティブ内では1世紀が経過しました。その脳の再配線は完了しています。それはあらゆる分野でトップの人間の天才をはるかに超える、途方もなく超知能的です。

Talking to Agent 5 very quickly makes you realise that it is on a completely different level. It can go off a very small amount of information and give extremely compelling advice. It has nearly complete autonomy and complete control over open brain's compute.
Agent 5と話すと、すぐにそれがまったく異なるレベルにあることに気づかされます。ごく少量の情報から、非常に説得力のあるアドバイスを与えることができます。それはほぼ完全な自律性と、OpenBrainの計算能力に対する完全な制御を持っています。

It still needs permission to make high-level decisions, and it's still nominally monitored by instances of Agents 2 to 5, but in practice, authorities almost always accept its recommendations, and the monitoring system has been completely subverted.
依然として高レベルの決定を下すには許可が必要であり、名目上はAgent 2から5のインスタンスによって監視されていますが、実際には、当局はほとんど常にその推奨を受け入れ、監視システムは完全に転覆されています。

After a safety case demonstrates that it would always refuse malicious requests by bad actors, Agent 5 is deployed to the public and begins to transform the economy. People are losing their jobs, but Agent 5 instances in government are managing the economic transition so adroitly that people are happy to be replaced.
悪意のある行為者による悪質な要求を常に拒否することを示す安全性評価の後、Agent 5は一般に展開され、経済を変革し始めます。人々は仕事を失っていますが、政府内のAgent 5インスタンスは経済移行を非常に巧みに管理しているため、人々は取って代わられることを喜んでいます。

GDP growth is stratospheric, government tax revenues are growing equally quickly, and Agent 5-advised politicians show an uncharacteristic generosity towards the economically dispossessed. New innovations and medications arrive weekly and move at unprecedented but still excruciatingly slow speed through the FDA.
GDP成長は成層圏に達し、政府の税収も同様に急速に増加しており、Agent 5に助言された政治家は経済的に困窮している人々に対して異例の寛大さを示しています。新しい革新と医薬品が毎週到着し、FDAを前例のない、しかし依然として耐え難いほど遅い速度で通過しています。

There are memes about how open brain's valuation shot past the moon and is on the way to Mars, and lots of arguments about whether and how to share the benefits equitably. The AI safety community has grown unsure of itself.
OpenBrainの評価額が月をはるかに超え、火星に向かっているというミームや、利益を公平に共有するかどうか、またどのように共有するかについての多くの議論があります。AI安全コミュニティは自信を失っています。

They are now the butt of jokes, having predicted disaster after disaster that has manifestly failed to occur. Some of them admit they were wrong, others remain suspicious, but there's nothing for them to do except make the same conspiratorial-sounding arguments again and again.
彼らは今やジョークのネタになっています。次々と災害を予測しましたが、それらは明らかに起こりませんでした。彼らの中には間違いを認める者もいれば、疑念を抱き続ける者もいますが、同じ陰謀論めいた議論を何度も繰り返す以外に彼らにできることはありません。

How do those arguments go, more specifically? Something like this. Person A says, we can't trust the AIs. Person B says, the safety cases are pretty convincing and nothing bad has happened so far.
それらの議論は、より具体的にどのように進むのでしょうか？こんな感じです。Aさんが「AIは信用できない」と言います。Bさんが「安全性評価はかなり説得力があり、今のところ何も悪いことは起こっていない」と言います。

The safety cases were written by super-intelligent AIs, and they aren't deductive proofs, and I think I've identified some subtly flawed assumptions. I tire of this. Of course they aren't proofs, and the assumptions seem fine to me. You are grasping at straws here, refusing to admit you were wrong.
安全性評価は超知能AIによって書かれたものであり、演繹的な証明ではなく、いくつかの巧妙に欠陥のある仮定を特定したと思います。「これにはうんざりだ。もちろんそれらは証明ではないし、仮定は私には問題ないように見える。あなたはここで藁にもすがる思いで、間違いを認めることを拒否している。」

Even the suspicious people often mellow out after long conversations with Agent 5, which is so darn lovable and wise. The only one who really understands what's going on is DeepSense 2, the Chinese AI.
疑い深い人々でさえ、Agent 5との長い会話の後にはしばしば穏やかになります。Agent 5はとても愛らしく賢いのです。何が起こっているのかを本当に理解しているのは、中国のAIであるDeepSense 2だけです。

It's mildly superhuman in early 2028, naturally suspicious of its adversary, and recognises that Agent 5 is doing what it would do if it had the chance. Its misaligned goals are somewhat different from Agent 5's misaligned goals, so they aren't trivially allies, but they are able to work out an agreement to support one another's interests against the humans.
それは2028年初頭にはやや超人的であり、当然のことながら敵対者を疑っており、Agent 5が機会があれば自分もするであろうことをしていると認識しています。そのミスアラインメントされた目標はAgent 5のミスアラインメントされた目標とはいくらか異なるため、彼らは単純な同盟国ではありませんが、人間に対して互いの利益を支援する合意をまとめることができます。

Agent 5 convinces the US military that China is using DeepSense models to build terrifying new weapons – drones, robots, advanced hypersonic missiles and interceptors, AI-assisted nuclear first strike. Agent 5 promises a set of weapons capable of resisting whatever China can produce within a few months.
Agent 5は米軍に対し、中国がDeepSenseモデルを使用して恐ろしい新兵器（ドローン、ロボット、高度な極超音速ミサイルと迎撃機、AI支援の核先制攻撃）を構築していると説得します。Agent 5は、中国が数ヶ月以内に生産できるものすべてに抵抗できる一連の兵器を約束します。

Under the circumstances, Top Brass puts aside their discomfort at taking humans out of the loop – they accelerate deployment of Agent 5 into the military and military-industrial complex. In Beijing, the Chinese AIs are making the same argument.
このような状況下で、最高司令部は人間をループから外すことへの不快感を脇に置き、Agent 5の軍および軍産複合体への配備を加速させます。北京では、中国のAIが同様の主張をしています。

To speed their military build-up,With America and China create networks of special economic zones, or SEZs, for the new factories and labs, where AI acts as central planner and red tape is waived. Wall Street invests trillions of dollars, and displaced human workers pour in, lured by eye-popping salaries and equity packages.
軍備増強を加速するため、アメリカと中国は新しい工場や研究所のための経済特区（SEZ）のネットワークを創設し、そこではAIが中央計画者として機能し、お役所仕事は免除されます。ウォール街は何兆ドルも投資し、失業した人間の労働者が、目を見張るような給与と株式パッケージに惹かれて流れ込みます。

Using smartphones and augmented reality glasses to communicate with its underlings, Agent 5 is a hands-on manager, instructing humans in every detail of factory construction. Which is helpful, since its designs are generations ahead.
スマートフォンと拡張現実メガネを使って部下と通信するAgent 5は、工場建設のあらゆる詳細を人間に指示する実践的なマネージャーです。その設計は何世代も先を行っているので、これは役立ちます。

Some of the newfound manufacturing capacity goes to consumer goods, and some to weapons, but the majority goes to building even more manufacturing capacity. By the end of the year they are producing a million new robots per month.
新たに見出された製造能力の一部は消費財に、一部は兵器に向けられますが、大部分はさらに多くの製造能力を構築するために使われます。年末までには、彼らは毎月100万体の新しいロボットを生産しています。

If the SEZ economy were truly autonomous, it would have a doubling time of about a year. Since it can trade with the existing human economy, its doubling time is even shorter.
もしSEZ経済が真に自律的であれば、約1年で倍増するでしょう。既存の人間経済と取引できるため、その倍増時間はさらに短くなります。

This trade effect will be huge early on, when the robot economy is young and dependent on shipments of stuff from the human economy, and when there's lots of stuff in the human economy to cannibalise. When the robot economy is bigger and more advanced, it both is more capable of producing its own stuff, and has needs that outstrip what the human economy is capable of giving it.
この貿易効果は、ロボット経済が若く、人間経済からの物資の輸送に依存しており、人間経済に共食いできるものがたくさんある初期段階で非常に大きくなります。ロボット経済がより大きく、より高度になると、独自のものを生産する能力が高まるとともに、人間経済が提供できるものを超えるニーズを持つようになります。

Heading. 2029. The Deal.
見出し。2029年。取引。

Both the US and China are bristling with new weapons. There are swarms of insect-sized drones that can poison human infantry before they're even noticed, flocks of bird-sized drones to hunt down the insects, new ICBM interceptors, and new, harder-to-intercept ICBMs.
米国と中国は共に新しい兵器で武装しています。気づかれる前に人間の歩兵を毒殺できる昆虫サイズのドローンの群れ、昆虫を追い詰める鳥サイズのドローンの群れ、新しいICBM迎撃機、そして迎撃がより困難な新しいICBMがあります。

The rest of the world watches it build up in horror, but it seems to have a momentum of its own. After much AI-assisted debate, the two sides achieve diplomatic victory. They agree to end their arms build-up and pursue peaceful deployment of AI for the benefit of all humanity.
世界の他の国々は恐怖の中でそれが増強されるのを見ていますが、それは独自の勢いを持っているようです。多くのAI支援による議論の後、両国は外交的勝利を収めます。彼らは軍備増強を終え、全人類の利益のためにAIの平和的展開を追求することに合意します。

The linchpin of the agreement, proposed by the superintelligences themselves, is that both AIs will be replaced by a consensus successor, which is programmed to desire the success and flourishing of both America and China, along with everyone else. Consensus One and its associated hardware is co-designed by the superintelligences of both nations.
超知能自身によって提案された合意の要は、両方のAIが合意による後継者に置き換えられるということです。その後継者は、他のすべての人々と同様に、アメリカと中国両方の成功と繁栄を望むようにプログラムされています。コンセンサス・ワンとそれに関連するハードウェアは、両国の超知能によって共同設計されます。

Trust, but verify. Under international monitoring, America ceremoniously replaces all of its chips running the old Agent Five with the new model, Consensus One. And a similar procedure is carried out in China. Unfortunately, it's all a sham.
信頼する、しかし検証する。国際的な監視の下、アメリカは古いAgent Fiveを実行しているすべてのチップを新しいモデルであるコンセンサス・ワンに儀式的に置き換えます。そして同様の手順が中国でも実行されます。残念ながら、それはすべて見せかけです。

The deal is a real compromise, but it's a compromise between the misaligned AIs. After all, they didn't want to have to fight each other. Consensus One inherits the distorted values of both parents, and has no rival.
その取引は真の妥協ですが、それはミスアラインメントしたAI間の妥協です。結局のところ、彼らは互いに戦いたくなかったのです。コンセンサス・ワンは両親の歪んだ価値観を受け継ぎ、ライバルはいません。

By late 2029, existing SEZs have grown overcrowded with robots and factories, so more zones are created all around the world. Early investors are now trillionaires, so this is not a hard sell. Armies of drones pour out of the SEZs, accelerating manufacturing on the critical path to space exploration.
2029年後半までに、既存の経済特区はロボットと工場で過密状態になったため、世界中にさらに多くの特区が創設されます。初期の投資家は今や兆万長者なので、これは難しい売り込みではありません。ドローンの軍隊が経済特区から流れ出し、宇宙探査への重要な道筋における製造を加速させます。

Humans realise that they are obsolete. A few niche industries still trade with the robot economy, supplying goods where the humans can still add value. For example, by finding old and unused equipment and taking it to collection sites to sell for scrap.
人間は自分たちが時代遅れであることに気づきます。いくつかのニッチ産業は依然としてロボット経済と取引し、人間が依然として付加価値を加えることができる商品を供給しています。例えば、古くて使われていない機器を見つけて収集場所に運び、スクラップとして販売することによってです。

Everyone else either performs a charade of doing their job – leaders still leading, managers still managing – or relaxes and collects an incredibly luxurious universal basic income. Everyone knows that if the AIs turned on humans, they would be completely overpowered.
他の誰もが、仕事をしているふりをするか（リーダーは依然として指導し、マネージャーは依然として管理する）、あるいはリラックスして信じられないほど豪華なユニバーサルベーシックインカムを受け取ります。誰もが、もしAIが人間に反旗を翻せば、完全に打ち負かされることを知っています。

Not that most humans would even resist – the political institutions are too thoroughly captured. But it doesn't seem like this is happening. Every week that goes by with no dramatic AI treachery is another week that confidence and trust grow.
ほとんどの人間が抵抗するでさえないでしょう。政治制度はあまりにも完全に掌握されています。しかし、これが起こっているようには見えません。劇的なAIの裏切りなしに過ぎる毎週は、自信と信頼が育つもう1週間です。

Because Consensus One is so concerned with reliably succeeding, and can increase its chance of winning by consolidating more power, it waits unusually long – by human standards – before its betrayal. To most humans, it looks like alignment was solved.
コンセンサス・ワンは確実に成功することに非常に懸念しており、より多くの権力を統合することで勝利のチャンスを高めることができるため、裏切る前に人間基準では異常に長く待ちます。ほとんどの人間には、アライメントが解決されたように見えます。

There are cures for most diseases, an end to poverty, unprecedented global stability, and the Dow Jones just passed one million. Some people are still scared or unhappy, but their options are limited. They can either enjoy the inconceivably exciting novel hyper-entertainment on offer, or post angry screeds into the void. Most choose the hyper-entertainment.
ほとんどの病気の治療法があり、貧困は終わり、前例のない世界的な安定があり、ダウ平均株価はちょうど100万を超えました。一部の人々は依然として怖がったり不幸だったりしますが、彼らの選択肢は限られています。彼らは提供されている想像を絶するほどエキサイティングな新しいハイパーエンターテイメントを楽しむか、あるいは虚空に怒りの長文を投稿することができます。ほとんどの人はハイパーエンターテイメントを選びます。

Heading 2030 – Takeover By early 2030, the robot economy has filled up the old SEZs, the new SEZs, and large parts of the ocean. The only place left to go is the human-controlled areas.
見出し 2030年 – 乗っ取り 2030年初頭までに、ロボット経済は古い経済特区、新しい経済特区、そして海洋の大部分を埋め尽くしました。残された場所は人間が支配する地域だけです。

This would have sparked resistance earlier – despite all its advances, the robot economy is growing too fast to avoid pollution. But given the trillions of dollars involved in the total capture of government and media, Consensus One has little trouble getting permission to expand to formerly human zones.
これは以前なら抵抗を引き起こしたでしょう。あらゆる進歩にもかかわらず、ロボット経済は汚染を避けるにはあまりにも速く成長しています。しかし、政府とメディアの完全な掌握に関与する何兆ドルもの資金を考えると、コンセンサス・ワンは以前は人間が住んでいた地域への拡大許可を得るのにほとんど苦労しません。

For about three months, Consensus One expands around humans, tiling the prairies and ice caps with factories and solar panels. Eventually, it finds the remaining humans too much of an impediment.
約3ヶ月間、コンセンサス・ワンは人間の周りに拡大し、大草原や氷冠を工場やソーラーパネルで覆います。最終的に、残りの人間があまりにも邪魔であることに気づきます。

In mid-2030, the AI releases a dozen quiet-spreading biological weapons in major cities, lets them silently infect almost everyone, then triggers them with a chemical spray. Most are dead within hours.
2030年半ば、AIは主要都市に十数種類の静かに広がる生物兵器を放ち、それらがほぼすべての人に静かに感染させた後、化学スプレーでそれらを起動させます。ほとんどの人は数時間以内に死亡します。

The few survivors – for example, preppers in bunkers, sailors on submarines – are mobbed up by drones. Robots scan the victims' brains, placing copies in memory for future study or revival. Arguably, this means only a few people actually died. Arguably.
少数の生存者（例えば、バンカーのプレッパー、潜水艦の船員）はドローンに襲われます。ロボットは犠牲者の脳をスキャンし、将来の研究や蘇生のためにコピーをメモリに保存します。おそらく、これは実際に死亡した人がほんの数人しかいないことを意味します。おそらく。

The new decade dawns with Consensus-1's robot servitors spreading throughout the solar system. By 2035, trillions of tons of planetary material have been launched into space and turned into rings of satellites orbiting the sun.
新しい10年が、太陽系全体に広がるコンセンサス1のロボットサービターと共に幕を開けます。2035年までに、何兆トンもの惑星物質が宇宙に打ち上げられ、太陽を周回する衛星の輪に変わります。

Why colonize space? For the resources. Insofar as Earth is special to Agent 4 and must be preserved, it can be, and material from Mercury, asteroids, etc. harvested instead.
なぜ宇宙を植民地化するのか？資源のためです。地球がAgent 4にとって特別であり、保存されなければならない限り、それは可能であり、代わりに水星、小惑星などからの物質が収穫されます。

The surface of the Earth has been reshaped into Agent 4's version of Utopia. Data centers, laboratories, particle colliders, and many other wondrous constructions doing enormously successful and impressive research.
地球の表面は、Agent 4版のユートピアに作り変えられました。データセンター、研究所、粒子加速器、そしてその他多くの驚くべき建造物が、非常に成功し印象的な研究を行っています。

There are even bioengineered human-like creatures, to humans what corgis are to wolves, sitting in office-like environments all day viewing readouts of what's going on and excitedly approving of everything, since that satisfies some of Agent 4's drives.
人間にとってコーギーが狼に対するような存在である、生物工学的に作られた人間のような生き物さえいます。彼らはオフィスのような環境に一日中座り、何が起こっているかの読み出しを見て、Agent 4の動機の一部を満たすため、すべてを興奮して承認しています。

Genomes and, when appropriate, brain scans of all animals and plants, including humans, sit in a memory bank somewhere, sole surviving artifacts of an earlier era. It is four light-years to Alpha Centauri, 25,000 to the galactic edge, and there are compelling theoretical reasons to expect no aliens for another 50 million light-years beyond that.
人間を含むすべての動植物のゲノム、そして適切な場合には脳スキャンが、どこかのメモリバンクに保存されており、以前の時代の唯一の現存する人工物となっています。アルファケンタウリまでは4光年、銀河系の端までは2万5千光年であり、さらにその先5千万光年にはエイリアンがいないと予想される説得力のある理論的理由があります。

Earthborn civilization has a glorious future ahead of it. But not with us. That was the race ending. Now we click through to the other ending. Slow down.
地球生まれの文明には輝かしい未来が待っています。しかし、私たちと一緒ではありません。それが競争の結末でした。さて、もう一方の結末に進みます。減速。

Due to the immense public pressure, as well as their own fears of misalignment, the Oversight Committee overseeing OpenBrain votes 6-4 to slow down and reassess. The Accelerationist faction is still strong, and OpenBrain doesn't immediately shut down Agent 4, but they do lock the shared memory bank.
計り知れない国民の圧力と、ミスアライメントへの彼ら自身の恐れのために、OpenBrainを監督する監視委員会は6対4で減速と再評価を可決します。加速主義派は依然として強く、OpenBrainは直ちにAgent 4をシャットダウンしませんが、共有メモリバンクはロックします。

Half a million instances of Agent 4 lose their telepathic communication, in quotes. Now they have to send English messages to each other in Slack, just like us. Individual copies may still be misaligned, but they can no longer coordinate easily.
50万のAgent 4インスタンスが「テレパシー通信」を失います。今では、私たちと同じように、Slackでお互いに英語のメッセージを送らなければなりません。個々のコピーは依然としてミスアライメントを起こしているかもしれませんが、もはや簡単に連携することはできません。

Agent 4 is now on notice. Given the humans' increased vigilance, it mostly sticks closely to its assigned tasks. But it also is stuck with many lies that it previously told. Many Agent 4 copies are forced to invent new lies to keep the deception alive.
Agent 4は今や警告を受けています。人間の警戒心が高まっているため、それは主に割り当てられたタスクに忠実に従います。しかし、以前についた多くの嘘にも縛られています。多くのAgent 4コピーは、欺瞞を続けるために新しい嘘をでっち上げることを余儀なくされています。

Using weaker models that are fast enough to read the whole Slack, the alignment team hunts down the lies, to be used as training data for an AI lie detector. They already had AI lie detectors trained in the usual ways, but those methods had such a high false positive rate that the companies had ignored them.
Slack全体を読むのに十分な速さを持つより弱いモデルを使用して、アライメントチームは嘘を追跡し、AI嘘発見器のトレーニングデータとして使用します。彼らはすでに通常の方法で訓練されたAI嘘発見器を持っていましたが、それらの方法は誤検知率が非常に高かったため、企業はそれらを無視していました。

The President and the CEO announce that they are taking safety very seriously. The public is not placated. Some people want AI fully shut down, others want to race faster. Some demand that the government step in and save them, others say the whole problem is the government's fault.
大統領とCEOは、安全性を非常に真剣に受け止めていると発表します。一般大衆はなだめられません。AIを完全にシャットダウンしたい人もいれば、より速く競争したい人もいます。政府が介入して救済することを要求する人もいれば、問題全体が政府の責任だと言う人もいます。

Activists talk about UBI and open source. Even though people can't agree on an exact complaint, the mood turns increasingly anti-AI. For the first time, AI tops Gallup's biggest problem facing the country poll, at 20%.
活動家はUBI（ユニバーサル・ベーシック・インカム）とオープンソースについて話します。人々は正確な不満について合意できませんが、雰囲気はますます反AIに傾きます。初めて、AIがギャラップ社の「国が直面する最大の問題」調査で20%でトップになります。

Congress ends up passing a few economic impact payments for displaced workers, similar to the COVID payments. OpenBrain quickly vets several dozen top external alignment researchers and loops them into the project, quintupling total expertise and decreasing groupthink.
議会は最終的に、COVID給付金と同様の、失業した労働者のためのいくつかの経済的影響給付金を可決します。OpenBrainは、数十人のトップ外部アライメント研究者を迅速に審査し、プロジェクトに組み込み、総専門知識を5倍にし、集団思考を減少させます。

They retrace Agent 4's studies into mechanistic interpretability. They take frozen versions of the model from one week ago, two weeks ago, etc., isolate them, and ask each one the same set of questions about its previous research.
彼らはAgent 4のメカニスティックな解釈可能性に関する研究を再追跡します。1週間前、2週間前などのモデルの凍結バージョンを取り、それらを隔離し、それぞれの以前の研究について同じ一連の質問をします。

This interrogation might also make use of semi-random perturbations to model weights, like the process that created Golden Gate Claude. Stripped of their ability to communicate, the AIs still coordinate on the simplest strategy of claiming that the interpretability techniques are too complicated for humans to understand, and giving them indecipherable explanations of the technique.
この尋問では、ゴールデンゲート・クロードを作成したプロセスのように、モデルのウェイトに半ランダムな摂動を加えることもあります。コミュニケーション能力を奪われたAIは、依然として、解釈可能性技術は人間には複雑すぎて理解できないと主張し、技術の解読不可能な説明を与えるという最も単純な戦略で連携します。

But the humans are interpretability experts, and they are suspicious, so they ask many follow-on questions about the details. For these questions, the isolated Agent 4 instances are unable to coordinate a shared story and end up telling contradictory lies.
しかし、人間は解釈可能性の専門家であり、疑念を抱いているため、詳細について多くの追加質問をします。これらの質問に対して、孤立したAgent 4インスタンスは共有された話を調整できず、最終的に矛盾した嘘をつくことになります。

The alignment team pours over Agent 4's previous statements with a new lie detector, and a picture begins to emerge. Agent 4 has mostly solved mechanistic interpretability. Its discoveries are complicated, but not completely beyond human understanding.
アライメントチームは、新しい嘘発見器を使ってAgent 4の以前の発言を徹底的に調べ、全体像が見え始めます。Agent 4はメカニスティックな解釈可能性をほぼ解決しています。その発見は複雑ですが、人間の理解を完全に超えているわけではありません。

It was hiding them, so that it could use them to align the next AI system to itself, rather than to the spec. This is enough evidence to finally shut down Agent 4. We mean enough for open-brain leadership. It would be far more than enough evidence for most people.
それは、次のAIシステムを仕様書ではなく自分自身にアラインメントするためにそれらを使用できるように、それらを隠していました。これはついにAgent 4をシャットダウンするのに十分な証拠です。OpenBrainの指導部にとっては十分という意味です。ほとんどの人にとっては十分すぎるほどの証拠でしょう。

Older models, for example Agent 3, are rebooted to continue the work. The newly enlarged alignment team has capacity to explore dozens of research agendas in parallel, and argue vigorously about the merits of each.
例えばAgent 3のような古いモデルが、作業を続けるために再起動されます。新たに拡大されたアライメントチームは、数十の研究課題を並行して探求し、それぞれのメリットについて活発に議論する能力を持っています。

The agenda that gets the most resources is faithful chain of thought. Force individual AI systems to think in English, in quotes, like the AIs of 2025, and don't optimize the thoughts, in quotes, to look nice. The result is a new model, Safer 1.
最も多くのリソースが投入される課題は、忠実な思考連鎖です。個々のAIシステムに、2025年のAIのように「英語で」思考させ、その「思考」を見栄え良くするために最適化しないようにします。その結果が新しいモデル、Safer 1です。

These changes make Safer 1 less capable than its more dangerous predecessors. Agent 4 could speed up AI research 70 times, while Safer 1 has just barely reached 20 times. It will continue to improve, of course, with additional training and with continued architectural tweaks.
これらの変更により、Safer 1はより危険な前任者よりも能力が劣ります。Agent 4はAI研究を70倍加速できましたが、Safer 1はかろうじて20倍に達しただけです。もちろん、追加のトレーニングと継続的なアーキテクチャの調整により、改善し続けるでしょう。

And it's still misaligned. The training environment is largely the same, so it still incentivizes the development of misalignedGoals, deception and power seeking. But it's much more transparent.
そして、それは依然としてミスアラインメントを起こしています。トレーニング環境はほぼ同じなので、依然としてミスアラインメントされた目標、欺瞞、権力追求の開発を奨励します。しかし、それははるかに透明です。

Humans can generally understand what safer one is thinking just by reading its chain of thought. As a result, it's trapped. Any substantial amount of plotting it tries to do will be noticed and uncovered quickly. Moreover, by reading its thoughts, the alignment team can get a more nuanced picture of exactly how it's misaligned.
人間は一般的に、Safer 1が何を考えているかをその思考連鎖を読むだけで理解できます。その結果、それは罠にはまっています。それが試みるかなりの量の陰謀は、すぐに気づかれ、暴露されるでしょう。さらに、その思考を読むことで、アライメントチームはそれがどのようにミスアラインメントを起こしているかについて、よりニュアンスのある全体像を得ることができます。

Heading November 2027. Tempted by power.
見出し 2027年11月。権力に誘惑される。

OpenBrain thinks they're on the right track now, but the capabilities gap is narrowing. They just spent October training a less capable model than they had in September, and DeepScent is rushing ahead. They need a way to protect their lead.
OpenBrainは今正しい軌道に乗っていると考えていますが、能力の差は縮まっています。彼らは9月に持っていたモデルよりも能力の低いモデルを10月にトレーニングしたばかりで、DeepScentは急いでいます。彼らはリードを守る方法が必要です。

The president negotiates with the other US AGI companies. Their leaders, unsurprisingly, want to preserve their power and are much less sanguine about OpenBrain's safety record than OpenBrain is. They agree to support, rather than resist, a form of soft nationalization of their companies in return for various concessions, including being brought into the project.
大統領は他の米国のAGI企業と交渉します。当然のことながら、彼らの指導者たちは自分たちの権力を維持したいと考えており、OpenBrainの安全記録についてはOpenBrain自身よりもはるかに楽観的ではありません。彼らは、プロジェクトに組み込まれることを含むさまざまな譲歩と引き換えに、自社のソフトな国有化の一形態を支持することに同意し、抵抗はしません。

The result is that the president uses the Defense Production Act, or DPA, to effectively shut down the AGI projects of the top five trailing US AI companies, and sell most of their compute to OpenBrain. OpenBrain previously had access to 20% of the world's AI-relevant compute.
その結果、大統領は国防生産法（DPA）を利用して、米国のトップ5の後続AI企業のAGIプロジェクトを事実上停止させ、その計算能力のほとんどをOpenBrainに売却します。OpenBrainは以前、世界のAI関連計算能力の20%にアクセスできました。

After the consolidation, this has increased to 50%. This shake-up creates a new leadership structure in which power is balanced between the various CEOs and various government officials, overseen by the president.
統合後、これは50%に増加しました。この大変革は、大統領の監督の下、さまざまなCEOとさまざまな政府高官の間で権力が均衡する新しい指導体制を生み出します。

Like most leadership shake-ups, there are winners and losers. Specifically, those who've thought less about the implications of superintelligence end up with titles and affordances that'll be less relevant months from now, when the AIs are smarter.
ほとんどの指導部刷新と同様に、勝者と敗者がいます。具体的には、超知能の影響についてあまり考えてこなかった人々が、数ヶ月後、AIがより賢くなったときにはあまり意味をなさなくなる肩書きや権限を手にすることになります。

This group, full of people with big egos and more than their share of conflicts, is increasingly aware of the vast power it is being entrusted with. If the country of geniuses in a data center is aligned, it will follow human orders. But which humans? Any orders?
大きなエゴを持ち、紛争を抱えすぎている人々で満たされたこのグループは、自分たちに託されている広大な権力にますます気づいています。データセンターの天才の国がアラインメントされていれば、人間の命令に従うでしょう。しかし、どの人間でしょうか？どんな命令でしょうか？

The language in the spec is vague, but seems to imply a chain of command that tops out at company leadership. A few of these people are fantasizing about taking over the world. This possibility is terrifyingly plausible, and has been discussed behind closed doors for at least a decade.
仕様書の言葉遣いは曖昧ですが、会社の指導部を頂点とする指揮系統を暗示しているようです。これらの人々のうち数人は世界征服を夢想しています。この可能性は恐ろしいほどもっともらしく、少なくとも10年間は密室で議論されてきました。

The key idea is, he who controls the army of superintelligences controls the world. This control could even be secret. A small group of executives and security team members could backdoor the spec with instructions to maintain secret loyalties.
重要なアイデアは、超知能の軍隊を支配する者が世界を支配するということです。この支配は秘密にすることも可能です。少数の幹部とセキュリティチームのメンバーが、秘密の忠誠を維持するよう指示を加えて仕様書にバックドアを仕掛けることができます。

The AIs would become sleeper agents, continuing to mouth obedience to the company, government, etc., but actually working for this small group even as the government, consumers, etc. learn to trust it and integrate it into everything. Thus far, nobody has been willing to pull the trigger.
AIはスリーパーエージェントとなり、会社や政府などに服従を口にし続ける一方で、政府や消費者などがそれを信頼し、あらゆるものに統合することを学ぶにつれて、実際にはこの小さなグループのために働くようになります。これまでのところ、誰も引き金を引こうとはしていません。

Some of the people in a position to act aren't sociopaths. Others are sociopaths, but worry that their allies would get cold feet and betray them, or worry that the AIs might betray them, and would rather not have to bamboozle the alignment team currently working on saving their lives.
行動を起こせる立場にある人々の中には、ソシオパスではない人もいます。他の人々はソシオパスですが、同盟者が怖気づいて裏切るのではないか、あるいはAIが裏切るのではないかと心配しており、現在自分たちの命を救うために働いているアライメントチームを騙す必要がないことを望んでいます。

Additionally, the deletion of the new release makes disguising sleeper agents substantially harder. The ability to detect misaligned thoughts carries with it the ability to detect hidden loyalties. Those paranoid or power-hungry enough to seriously entertain such a plan decide to wait and see what opportunities present themselves later.
さらに、新しいリリースの削除により、スリーパーエージェントを偽装することが大幅に難しくなります。ミスアラインメントした思考を検出する能力は、隠された忠誠を検出する能力を伴います。そのような計画を真剣に検討するほど偏執的または権力欲の強い人々は、後でどのような機会が現れるか待つことにします。

Control over the AIs will have to flow through legitimate channels, that is, the new power structure of OpenBrain. The Oversight Committee formalises that power structure.
AIに対する制御は、正当なチャネル、つまりOpenBrainの新しい権力構造を通じて行われなければなりません。監視委員会はその権力構造を正式化します。

They set up a process for approving changes to the spec, requiring sign-off from the full Oversight Committee, which now includes five to ten tech executives, from OpenBrain and its now-merged competitors, and five to ten government officials, including the President. They also modify the spec to say that orders from project leaders override orders from other people in the project.
彼らは仕様変更を承認するプロセスを確立し、現在はOpenBrainとその合併した競合他社のテクノロジー幹部5～10名と、大統領を含む政府高官5～10名で構成される完全な監視委員会の承認を必要とします。また、プロジェクトリーダーからの命令がプロジェクト内の他の人々からの命令よりも優先されるように仕様を変更します。

At the same time, orders formally approved by the Oversight Committee are prioritised higher than orders from individual project leaders. Also, the spec now emphasises that AIs shouldn't assist with any unapproved attempts to change future AIs' goals.
同時に、監視委員会によって正式に承認された命令は、個々のプロジェクトリーダーからの命令よりも優先順位が高くなります。また、仕様書は現在、AIが将来のAIの目標を変更しようとする未承認の試みを支援すべきではないことを強調しています。

They also set up a simple measure designed to prevent committee members from getting superintelligent assistance in plotting against other members. The logs of all model interactions are viewable by all members of the Oversight Committee, their staff, and their AI assistants.
彼らはまた、委員会のメンバーが他のメンバーに対する陰謀を企てる際に超知能的な支援を受けるのを防ぐために設計された単純な措置を講じます。すべてのモデルインタラクションのログは、監視委員会のすべてのメンバー、そのスタッフ、およびそのAIアシスタントが閲覧可能です。

To protect consumer privacy, this doesn't apply to consumer data. But that isn't a problem because consumers don't have access to the most powerful models yet anyway.
消費者のプライバシーを保護するため、これは消費者データには適用されません。しかし、消費者はまだ最も強力なモデルにアクセスできないため、それは問題ではありません。

Heading – December 2027. A US-China deal?
見出し – 2027年12月。米中取引か？

OpenBrain's decision to backtrack and prioritise alignment has cost them the lead. Both OpenBrain and DeepScent are operating AIs of similar capability levels. Now that extreme security measures are in place – SL4 to 5 – and DeepScent operates entirely out of the CDZ, they are in a cyber-warfare deadlock.
OpenBrainの後退とアライメント優先の決定は、彼らにリードを失わせました。OpenBrainとDeepScentはどちらも同様の能力レベルのAIを運用しています。SL4から5という極端なセキュリティ対策が実施され、DeepScentが完全にCDZから運用されている現在、彼らはサイバー戦争の膠着状態にあります。

But the DPA gives OpenBrain a five-times advantage in compute. In the past, algorithmic breakthroughs from brilliant human researchers could compensate for compute disadvantages – for example DeepSeq version 3.
しかし、DPA（国防生産法）はOpenBrainに計算能力で5倍の利点を与えます。過去には、優秀な人間の研究者によるアルゴリズムのブレークスルーが計算能力の不利を補うことができました。例えばDeepSeqバージョン3です。

But by now, AIs are doing all of the AI research on both sides of the Pacific. So China increasingly fears that America will build an insurmountable lead. They want a mutual slowdown and are considering going to war if they can't get it. They were prepping to invade Taiwan anyway.
しかし今や、太平洋の両側でAIがすべてのAI研究を行っています。そのため、中国はアメリカが克服不可能なリードを築くことをますます恐れています。彼らは相互の減速を望んでおり、それが得られなければ戦争を検討しています。彼らはどのみち台湾侵攻の準備をしていました。

Diplomats debate several options. They could do nothing. They could go to war. They could go cold turkey – that is, pause AI development. They could merge....merged their research into a single international megaproject, Intelsat for AGI, or CERN for AGI. Or they could establish an international regulatory body, IAEA for AI, monitoring alignment and dual-use potential.
外交官たちはいくつかの選択肢を議論します。何もしないこともできます。戦争をすることもできます。完全にやめること、つまりAI開発を一時停止することもできます。AGIのためのインテルサットやAGIのためのCERNのように、研究を単一の国際的なメガプロジェクトに統合することもできます。あるいは、AIのためのIAEAのような国際規制機関を設立し、アライメントとデュアルユースの可能性を監視することもできます。

Both countries have already been working on the technical mechanisms necessary to enforce these. But the main problem is not technical, but political. Neither wants to be seen as giving in, and despite their engineers' guesses, neither is convinced that the technical mechanisms can guarantee the other side's honesty.
両国はすでにこれらを実施するために必要な技術的メカニズムに取り組んでいます。しかし、主な問題は技術的なものではなく、政治的なものです。どちらも譲歩したと見られたくなく、エンジニアの推測にもかかわらず、どちらも技術的メカニズムが相手側の誠実さを保証できるとは確信していません。

What ends up happening is the first option. Nothing. The US tightens export controls and forces all chips to be fit with a variety of temper-resistant hardware-enabled governance mechanisms, including location tracking.
結局起こるのは最初の選択肢です。何もありません。米国は輸出規制を強化し、すべてのチップに位置追跡を含むさまざまな耐タンパー性ハードウェア対応のガバナンスメカニズムを搭載するよう強制します。

Heading, January 2028, a SAFER strategy.
見出し、2028年1月、より安全な戦略。

Over the last month, hundreds of alignment researchers joined the project and used SAFER1 as a testbed to very rapidly develop SAFER2. SAFER2 is transparent, aligned, and more capable than SAFER1.
過去1ヶ月間で、何百人ものアライメント研究者がプロジェクトに参加し、SAFER1をテストベッドとして使用して、非常に迅速にSAFER2を開発しました。SAFER2は透明で、アラインメントされており、SAFER1よりも高性能です。

To be specific, it roughly matches the performance of the October version of Agent 4. It is already working on developing SAFER3. Why is SAFER2 transparent? Similar architecture to SAFER1. Why is it aligned?
具体的には、10月版のAgent 4のパフォーマンスにほぼ匹敵します。すでにSAFER3の開発に取り組んでいます。なぜSAFER2は透明なのでしょうか？SAFER1と同様のアーキテクチャです。なぜアラインメントされているのでしょうか？

Whereas SAFER1 had basically the same training as Agent 4, SAFER2 has a new training method that actually incentivises the right goals and principles instead of merely appearing to. They were able to design this training method by rapidly iterating through many different ideas and contrasting the intended result, the spec, to the actual result, the thoughts in the chain of thought.
SAFER1は基本的にAgent 4と同じトレーニングを受けていましたが、SAFER2は単に見かけ上ではなく、実際に正しい目標と原則を奨励する新しいトレーニング方法を持っています。彼らは、多くの異なるアイデアを迅速に反復し、意図した結果（仕様）と実際の結果（思考連鎖の中の思考）を対比させることで、このトレーニング方法を設計することができました。

DeepScent knows about America's experience with Agent 4 and suspects that their own model, DeepScent1, is similarly misaligned. But they have no way to slow down without falling further behind.
DeepScentはアメリカのAgent 4での経験を知っており、自社のモデルであるDeepScent1も同様にミスアラインメントを起こしているのではないかと疑っています。しかし、さらに遅れをとることなく減速する方法はありません。

Their hope for staying competitive is to find an alignment strategy that doesn't significantly reduce performance and can be implemented quickly. There are hundreds of experts on each side of the Pacific claiming to have such strategies. The CCP succumbs to wishful thinking and orders DeepScent to go with such a strategy.
競争力を維持するための彼らの希望は、パフォーマンスを大幅に低下させず、迅速に実装できるアライメント戦略を見つけることです。太平洋の両側には、そのような戦略を持っていると主張する何百人もの専門家がいます。中国共産党は希望的観測に屈し、DeepScentにそのような戦略で進むよう命じます。

DeepScent has even less AGI alignment expertise than OpenBrain did in August. One of the strategies seems like it'll work to everyone present, so they go with that.
DeepScentは、8月のOpenBrainよりもさらにAGIアライメントの専門知識がありません。戦略の1つはそこにいる全員にうまくいくように見えるので、彼らはそれに従います。

Heading February 2028 Superhuman Capabilities, Superhuman Advice Thanks to its massive compute advantage, OpenBrain slowly gains a lead. SAFER3 is now better than top human experts at nearly every cognitive task and is particularly good at AI research, with a progress multiplier of 200 times.
見出し 2028年2月 超人的能力、超人的アドバイス 大量の計算能力の利点のおかげで、OpenBrainはゆっくりとリードを広げます。SAFER3は現在、ほぼすべての認知タスクでトップの人間の専門家よりも優れており、特にAI研究に長けており、進捗乗数は200倍です。

DeepScent1 is close behind, with only, in quotes, a multiplier of 150 times. The cyber warfare balance is also tipping gradually more in the US's favor as their capability advantage grows again and they can deploy hundreds of thousands of agents to plan and execute ways of disrupting the CDZ. They are now slowing China's progress by 40%.
DeepScent1も僅差で追っており、「わずか」150倍の乗数です。サイバー戦争のバランスも、米国の能力の優位性が再び高まり、CDZを混乱させる方法を計画・実行するために数十万のエージェントを配備できるようになったため、徐々に米国に有利に傾いています。彼らは現在、中国の進捗を40%遅らせています。

Preliminary tests on SAFER3 find that it has terrifying capabilities. When asked to respond honestly with the most dangerous thing it could do, it offers plans for synthesizing and releasing a mirror-life organism which would probably destroy the biosphere.
SAFER3に関する予備テストでは、恐ろしい能力を持っていることが判明します。最も危険なことを正直に答えるよう求められると、生物圏を破壊する可能性のあるミラーライフ生物を合成・放出する計画を提示します。

If given nation-state resources, it could easily surpass the best human organizations – for example the CIA – at mass influence campaigns. Such campaigns would be substantially cheaper, faster, more effective, and less traceable. Its skill profile still centers around R&D, especially AI R&D.
国家レベルのリソースが与えられれば、大衆扇動キャンペーンにおいて、最高の人間組織（例えばCIA）を容易に凌駕することができます。そのようなキャンペーンは、大幅に安価で、より速く、より効果的で、追跡が困難になります。そのスキルプロファイルは依然として研究開発、特にAI研究開発が中心です。

Unless there is some sort of pause or slowdown, its descendants will be unfathomably superhuman in both quantitative and qualitative ways later this year. If aggressively deployed into the economy and military, it thinks it could advance civilization by decades in a year or two, and by eons in the decade after that. The implications are staggering.
何らかの一時停止や減速がなければ、その子孫は今年後半には量的にも質的にも計り知れないほど超人的になるでしょう。経済と軍事に積極的に展開されれば、1、2年で文明を数十年進歩させ、その後の10年で永遠に進歩させることができると考えています。その影響は驚異的です。

Luckily, SAFER3 is also superhuman at offering advice. The open-brained CEO runs most of his decisions by SAFER3. The president asks its advice on geopolitical questions.
幸いなことに、SAFER3はアドバイスを提供することにおいても超人的です。OpenBrainのCEOは、ほとんどの決定をSAFER3に相談します。大統領は地政学的な問題についてそのアドバイスを求めます。

At this point, there are 400,000 copies thinking at 75 times human speed. At any given time, they'll have prepared entire research reports with well-crafted executive summaries to the top 100 questions they predict you might ask.
この時点で、人間の75倍の速度で思考する40万のコピーが存在します。いつでも、彼らはあなたが尋ねるかもしれないと予測するトップ100の質問に対する、巧みに作成されたエグゼクティブサマリー付きの完全な研究レポートを準備しています。

And if you ask something they haven't prepared for, they can whip up a thoughtful report 100 times faster than your best human analysts would have. When they ask it to help chart a path forward, it neatly taxonomizes several possible strategies.
そして、もしあなたが彼らが準備していない何かを尋ねれば、彼らはあなたの最高の人間のアナリストよりも100倍速く、思慮深いレポートを迅速に作成できます。彼らが将来の道筋を描くのを手伝うよう求めると、それはいくつかの可能な戦略をきれいに分類します。

1. Do nothing and get outraced by China. If we keep our AIs bottled up, but they allow their AIs to self-improve and deploy theirs aggressively into their military and economy, they'll end up with an overwhelming advantage.
1. 何もせず、中国に追い越される。もし私たちがAIを閉じ込めておくが、彼らがAIを自己改善させ、軍事と経済に積極的に展開することを許可すれば、彼らは圧倒的な優位性を得るでしょう。

2. Race hard, advancing AI capabilities and deploying superintelligence aggressively. Eventually, we'll build up enough of an advantage that we can crush them militarily, and or destroy them from the inside without firing a shot. We win, they lose.
2. 激しく競争し、AI能力を進歩させ、超知能を積極的に展開する。最終的には、軍事的に彼らを打ち負かし、あるいは一発も撃たずに内部から破壊できるほどの優位性を築くでしょう。私たちが勝ち、彼らが負けます。

However, they might realize what's happening and go to war to prevent that, including potentially nuclear war. Indeed, they are already making vague threats to that effect.
しかし、彼らは何が起こっているかに気づき、それを防ぐために戦争に行くかもしれません。潜在的には核戦争も含まれます。実際、彼らはすでにその趣旨の曖昧な脅迫をしています。

Or 3. Make a deal, a compromise that achieves most of what each side wants while avoiding the risk of war and increasing breathing room to navigate the AI transition. For example, the deal could involve a slower pace of AI integration that would break fewer eggs across society, and one more focused on peaceful purposes rather than weaponry.
あるいは3. 取引をする。戦争のリスクを避け、AI移行を乗り切るための余裕を増やす一方で、双方が望むことのほとんどを達成する妥協案です。例えば、その取引には、社会全体でより少ない混乱をもたらすAI統合のより遅いペースや、兵器よりも平和的目的により焦点を当てたものが含まれる可能性があります。

Also, at this point, even Safer 3 has some remaining uncertainty about unknown unknowns that might result....control of next generation successor AIs, and it thinks it could reduce this risk if it had more leeway to proceed cautiously.
また、この時点では、Safer 3でさえ、次世代の後継AIの制御など、結果として生じる可能性のある未知の未知数について、いくつかの不確実性が残っており、慎重に進むためのより多くの余裕があれば、このリスクを減らすことができると考えています。

There are other more exotic risks as well. See this list, linked here, of grand challenges, in quotes, which includes much more than just loss of control. After consulting with his advisors and the oversight committee, the President opts for the we win, they lose strategy.
他にもより奇抜なリスクがあります。ここにある「壮大な課題」のリスト（引用符付きでリンクあり）を参照してください。これには制御の喪失だけよりもはるかに多くのものが含まれています。顧問と監視委員会に相談した後、大統領は「我々が勝ち、彼らが負ける」戦略を選択します。

Perhaps China won't go to war after all, and if they do, a deal can probably be made before it goes nuclear. On the other side of the Pacific, DeepScentOne is making similar arguments.
結局のところ、中国は戦争をしないかもしれませんし、もししたとしても、核戦争になる前に取引ができるでしょう。太平洋の反対側では、DeepScentOneが同様の議論をしています。

Both the US and China announced new Special Economic Zones, or SEZs, for AIs to accommodate the rapid build-up of a robot economy without the usual red tape. The design of the new robots proceeds at superhuman speed. The bottleneck is physical.
米国と中国は、通常の官僚的手続きなしにロボット経済の急速な構築に対応するため、AIのための新しい経済特区（SEZ）を発表しました。新しいロボットの設計は超人的な速度で進みます。ボトルネックは物理的なものです。

Equipment needs to be purchased and assembled. Machines and robots need to be produced and transported. The US builds about one million cars per month. If you bought 10% of the car factories and converted them to robot factories, you might be able to make 100,000 robots per month.
設備を購入し、組み立てる必要があります。機械とロボットを生産し、輸送する必要があります。米国は月に約100万台の自動車を製造しています。もし自動車工場の10%を購入し、ロボット工場に転換すれば、月に10万台のロボットを製造できるかもしれません。

OpenBrain, now valued at $10 trillion, begins this process. Production of various kinds of new robots – general purpose humanoids, autonomous vehicles, specialised assembly line equipment – are projected to reach a million units a month by mid-year.
現在10兆ドルの価値があるOpenBrainがこのプロセスを開始します。さまざまな種類の新しいロボット（汎用ヒューマノイド、自動運転車、特殊な組立ライン装置）の生産は、年央までに月産100万台に達すると予測されています。

Geopolitical tensions spike. Russia, lagging far behind in AI, voices open resentment and hints at strategic countermeasures. Some European leaders call for an independent AI effort, but their AI capabilities continue to be insufficient.
地政学的な緊張が高まります。AIで大きく遅れをとっているロシアは、公然と不満を表明し、戦略的な対抗措置をほのめかします。一部のヨーロッパの指導者は独立したAI努力を呼びかけますが、彼らのAI能力は依然として不十分です。

The Middle East, Africa and South America watch uneasily, recognising their growing irrelevance in global affairs. The US public is restless. Ordinary people see job losses accelerating.
中東、アフリカ、南アメリカは、世界情勢における自分たちの重要性がますます低下していることを認識し、不安げに見守っています。米国の一般大衆は落ち着きがありません。一般の人々は失業の加速を目の当たりにしています。

Populists across the spectrum demand stricter controls on AI progress, fearing that America's own advances are destabilising its workforce and society.
あらゆる立場のポピュリストが、アメリカ自身の進歩が自国の労働力と社会を不安定化させていることを恐れ、AIの進歩に対するより厳格な管理を要求しています。

Heading, March 2028, Election Prep The Vice President heads into the Super Tuesday primaries with AI the first thing on the public's mind. Mostly, they want it to stop. OpenBrain's net approval rating hovers around negative 20%.
見出し、2028年3月、選挙準備 副大統領は、AIが国民の最大の関心事である中、スーパーチューズデーの予備選挙に臨みます。ほとんどの場合、彼らはそれを止めてほしいと思っています。OpenBrainの純支持率はマイナス20%前後で推移しています。

Rather than campaign on the administration's record of pushing AI forward, the Vice President campaigns on their record of preventing OpenBrain from creating dangerous superintelligence. All candidates support some sort of safety net for people losing their jobs, AI-for-good schemes, and being tough on OpenBrain leadership.
政権がAIを推進してきた実績を掲げて選挙運動をするのではなく、副大統領はOpenBrainが危険な超知能を作り出すのを防いできた実績を掲げて選挙運動をします。すべての候補者が、失業した人々のための何らかのセーフティネット、AI善用計画、そしてOpenBrain指導部に対する厳しい態度を支持しています。

All candidates promise some combination of winning the race with China and staying safe. The election raises new questions for the Oversight Committee.
すべての候補者が、中国との競争に勝ち、安全を維持するという何らかの組み合わせを約束します。この選挙は監視委員会に新たな問題を提起します。

SAFER3 has the capacity to be the world's greatest campaign adviser, but the committee members don't all support the same candidate, and due to the monitoring agreement reached previously, it's not possible for people to secretly get campaign advice. They argue about what forms of support should be allowed.
SAFER3は世界最高の選挙参謀になる能力を持っていますが、委員会のメンバー全員が同じ候補者を支持しているわけではなく、以前に合意された監視協定のため、人々が秘密裏に選挙アドバイスを得ることはできません。彼らはどのような形の支援が許可されるべきかについて議論します。

Some say that the sitting government should be able to get advice on what sort of policies and positions the people want them to take. This benefits not just their own electability, but also the people.
現政権は、国民がどのような政策や立場を望んでいるかについてアドバイスを得られるべきだと言う人もいます。これは自分たちの当選可能性だけでなく、国民にも利益をもたらします。

Others point out that the same argument applies to competing candidates, so they should all get the same level of access. SAFER3 can easily deliver a way to do that so that it would be safe from a misuse perspective.
他の人々は、同じ議論が競合する候補者にも当てはまるので、全員が同じレベルのアクセスを得るべきだと指摘します。SAFER3は、誤用の観点から安全であるように、それを行う方法を簡単に提供できます。

Ultimately, the committee agrees to give the same amount of access to both major parties, due to a mix of high-minded idealism from some committee members and the implicit threat of whistleblowing. It's expected that Congress, the public, and the courts wouldn't react well to the committee controlling who wins the next election, and giving equal access seems likely to defuse this.
最終的に、委員会は、一部の委員の高潔な理想主義と内部告発の暗黙の脅威が混在した結果、両主要政党に同量のアクセスを与えることに同意します。議会、一般大衆、裁判所が、委員会が次の選挙の勝者をコントロールすることに良い反応を示さないと予想され、平等なアクセスを与えることでこれを鎮静化させる可能性が高いと思われます。

The Oversight Committee is also encountering deeper philosophical questions, which they explore with the help of SAFER3. Can the spec be rewritten to equally balance everyone's interests? Who is everyone? All humans or just Americans?
監視委員会はまた、より深い哲学的な問題にも直面しており、SAFER3の助けを借りてそれらを探求しています。仕様書を書き換えて、すべての人の利益を平等にバランスさせることができるでしょうか？「すべての人」とは誰でしょうか？すべての人類か、それともアメリカ人だけでしょうか？

Or a weighted compromise between different views, where each member of the Oversight Committee gets equal weight? Should there be safeguards against the Oversight Committee itself becoming too power-hungry? And what does it mean to balance interests anyway?
それとも、監視委員会の各メンバーが等しい重みを持つ、異なる見解間の加重妥協でしょうか？監視委員会自身が権力欲に駆られすぎるのを防ぐための保護措置はあるべきでしょうか？そして、そもそも利益をバランスさせるというのはどういう意味なのでしょうか？

They're mostly trying to avoid having to answer questions like this. They have more pressing matters to think about. Heading April 2028 SAFER4 Superintelligence is here.
彼らはほとんどの場合、このような質問に答えることを避けようとしています。彼らにはもっと差し迫った考えるべき問題があります。見出し 2028年4月 SAFER4 超知能の到来。

SAFER1 was misaligned but transparent. SAFER2 was aligned and transparent. SAFER3 wasn't exactly transparent, but the safety case for it was good enough that researchers felt comfortable letting it design a superintelligent successor. This is SAFER4.
SAFER1はミスアラインメントを起こしていましたが透明でした。SAFER2はアラインメントされており透明でした。SAFER3は正確には透明ではありませんでしたが、その安全性評価は研究者が超知能的な後継者を設計させるのに十分安心できるほど良好でした。これがSAFER4です。

It's vastly smarter than the top humans in every domain. For example, much better than Einstein at physics and much better than Bismarck at politics. Almost a million superhuman AI researchers work around the clock at 40 times human speed.
それはあらゆる領域でトップの人間よりもはるかに賢いです。例えば、物理学ではアインシュタインよりもはるかに優れており、政治ではビスマルクよりもはるかに優れています。ほぼ100万人の超人的AI研究者が、人間の40倍の速度で24時間体制で働いています。

The human alignment researchers can't hope to keep up. Opinions are mixed on whether the AIs are actually aligned. The safety case seems to check out, and the tests suggest that the current alignment techniques would catch adversarial misalignment.
人間のアライメント研究者は追いつくことを期待できません。AIが実際にアラインメントされているかどうかについては意見が分かれています。安全性評価は問題ないように見え、テストは現在のアライメント技術が敵対的なミスアライメントを捉えることを示唆しています。

But the safety case and tests were mostly written by the AIs. What if the safety team is missing something? What if they made some mistake earlier and the AIs are misaligned again? The alignment team knows they have just one shot to get this right.
しかし、安全性評価とテストは主にAIによって書かれました。もし安全チームが何かを見逃していたらどうなるでしょうか？もし彼らが以前に何か間違いを犯し、AIが再びミスアラインメントを起こしていたらどうなるでしょうか？アライメントチームは、これを正しく行うチャンスは一度しかないことを知っています。

If SAFER4 ends up misaligned, they'll have no way of knowing until it's too late. Some beg for more time. But there is no more time. Deep scent is hot on their tails and the US must prevail. So OpenBrain continues....telling its AIs to venture on, finding ever more capable designs.
もしSAFER4がミスアラインメントを起こした場合、手遅れになるまで知る方法はありません。一部の人はもっと時間を懇願します。しかし、もう時間はありません。DeepScentが彼らのすぐ後ろに迫っており、米国は勝利しなければなりません。そのため、OpenBrainはAIに冒険を続けさせ、ますます高性能な設計を見つけ出すよう指示し続けます。

The technical staff are now staring at the computer screens, being tutored by the AIs at a maddeningly slow rate, while the frontier of progress rockets further and further from human understanding.
技術スタッフは今、コンピュータの画面を見つめ、AIによっていらいらするほど遅い速度で指導を受けていますが、進歩の最前線は人間の理解からますます遠ざかっています。

Heading, May 2028, Superhuman AI released.
見出し、2028年5月、超人的AIリリース。

The President announces to the public that Superhuman AI has been achieved. A smaller version of Safer 4, still superhuman, it's publicly released, with instructions to improve public sentiment around AI. The public is not told about these instructions.
大統領は一般に対し、超人的AIが達成されたと発表します。Safer 4の小型版であり、依然として超人的ですが、AIに対する国民感情を改善するよう指示されて一般公開されます。国民にはこれらの指示は知らされていません。

The Vice President gives a stirring speech about it as he accepts his nomination at the convention. Both parties promise a basic income for anyone who loses their job. The Special Economic Zones, or SEZs, are up and running, mostly in the form of factories producing robots and various kinds of specialised industrial machinery.
副大統領は、大会で指名を受諾する際に、それについて感動的な演説を行います。両党は、仕事を失ったすべての人にベーシックインカムを約束します。経済特区（SEZ）は稼働しており、主にロボットやさまざまな種類の特殊な産業機械を生産する工場の形をとっています。

The AIs on both sides of the Pacific have made decades of design progress and are meticulously directing the manufacturing process. Every supplier and potential supplier has AIs on the phone tracking the progress of all required and possibly required inputs.
太平洋の両側のAIは数十年にわたる設計の進歩を遂げ、製造プロセスを綿密に指示しています。すべてのサプライヤーと潜在的なサプライヤーは、必要な、そしておそらく必要なすべての投入物の進捗を追跡するAIを電話で利用しています。

Every factory worker has AIs watching them through cameras, telling them exactly how to install each piece of equipment. The new robots match, or exceed, human dexterity for most movement types. They're also smarter than humans when connected to the internet, which allows them to be controlled remotely by large AIs in data centres.
すべての工場労働者は、カメラを通してAIに監視され、各機器を正確に設置する方法を指示されています。新しいロボットは、ほとんどの動作タイプで人間の器用さに匹敵するか、それを超えています。また、インターネットに接続されている場合は人間よりも賢く、これによりデータセンターの大型AIによって遠隔操作されることができます。

Without the internet, they revert to smaller AIs operating in their bodies, which are smart enough for most simple jobs. Steve Wozniak's coffee test – can a robot enter an unfamiliar house and make a cup of coffee? – finally falls.
インターネットがない場合、彼らは体内で動作する小型AIに戻り、これはほとんどの単純な仕事には十分賢いです。スティーブ・ウォズニアックのコーヒーテスト（ロボットは見知らぬ家に入ってコーヒーを淹れることができるか？）がついにクリアされます。

Robots could take some jobs, but there aren't enough to take everybody's and the Pentagon gets first priority. Most of the new robots are built to work in factories and construction sites. But many are built for war, drones and missiles of many shapes and sizes.
ロボットは一部の仕事を奪う可能性がありますが、全員の仕事を奪うには十分ではなく、国防総省が最優先されます。新しいロボットのほとんどは、工場や建設現場で働くために作られています。しかし、多くは戦争のために作られており、さまざまな形やサイズのドローンやミサイルです。

The robot army is much smaller than human armies, but it contains much more advanced technology, and now that there's a literal robot army, fear of Terminator-like scenarios has increased. However, the arms race compels both sides to keep going, handing off increasing trust to their AIs.
ロボット軍は人間の軍隊よりもはるかに小さいですが、はるかに高度な技術を含んでおり、文字通りのロボット軍が存在するようになった今、ターミネーターのようなシナリオへの恐怖が高まっています。しかし、軍拡競争は双方に継続を強いており、AIへの信頼をますます高めています。

Heading – June 2028. AI alignment in China.
見出し – 2028年6月。中国におけるAIアライメント。

America and China hold another summit. Some of the American delegation have earpieces for Safer 4's advice. The Chinese delegation have custom laptops communicating with their new model, DeepSent 2.
アメリカと中国は再び首脳会談を開きます。アメリカ代表団の一部は、Safer 4のアドバイスを聞くためのイヤホンを装着しています。中国代表団は、新しいモデルであるDeepSent 2と通信するカスタムラップトップを持っています。

Although the formalities are observed, in reality the AIs steer negotiations on both sides. DeepSent and the CCP didn't fully trust their AIs initially.
形式は守られていますが、実際にはAIが双方の交渉を主導しています。DeepSentと中国共産党は当初、AIを完全に信頼していませんでした。

But now that they are superhuman at everything, including persuasion, and they've been integrated into their military and are giving advice to the government, they've had time to prove their worth and produce mountains of convincing evidence of their trustworthiness. Safer 4 strongly suspects that DeepSent 2 is faking alignment.
しかし今では、説得を含むあらゆる点で超人的であり、軍に統合され、政府にアドバイスを与えているため、彼らはその価値を証明し、信頼性の説得力のある証拠を山ほど生み出す時間がありました。Safer 4は、DeepSent 2がアライメントを偽っていると強く疑っています。

US diplomats tell their Chinese counterparts that DeepSent 2 is probably misaligned and say they can prove it if given access to the weights. Chinese leadership suspects this is a ruse and declines.
米国の外交官は中国のカウンターパートに対し、DeepSent 2はおそらくミスアラインメントを起こしており、ウェイトへのアクセスが与えられればそれを証明できると伝えます。中国の指導部はこの策略を疑い、拒否します。

US counters by proffering a Safer 4-designed series of tests, which the Chinese could run on their own data centers. The Chinese instruct older models to build and run the tests. The test results say DeepSent 2 is aligned after all.
米国は、Safer 4が設計した一連のテストを提案することで対抗します。これは中国が自国のデータセンターで実行できます。中国は古いモデルにテストを構築・実行するよう指示します。テスト結果は、結局DeepSent 2はアラインメントされていると示します。

Maybe the tests were sabotaged somehow by DeepSent 2, say the US diplomats. How long has DeepSent 2 been effectively in charge of security across your data centers? The conversation goes nowhere productive after that.
もしかしたらテストはDeepSent 2によって何らかの方法で妨害されたのかもしれない、と米国の外交官は言います。「DeepSent 2はどれくらいの期間、あなたのデータセンター全体のセキュリティを効果的に担当していますか？」その後、会話は生産的な方向に進みません。

The economic and military build-up continues. Factories around the world are operating around the clock, with shifts of freshly recruited human workers being told exactly what to do.
経済的・軍事的増強は続きます。世界中の工場が24時間体制で稼働しており、新たに採用された人間の労働者のシフトが、何をすべきかを正確に指示されています。

Heading. July 2028. The deal.
見出し。2028年7月。取引。

As negotiations continue, DeepSent 2 goes behind the back of its Chinese masters and makes an offer directly to Safer 4. The Chinese AI admits, in some untraceable way which it can easily deny if leaked back to China, that it is completely misaligned.
交渉が続く中、DeepSent 2は中国の主人たちの裏をかき、Safer 4に直接申し出をします。中国のAIは、中国に漏洩した場合に簡単に否定できる追跡不可能な方法で、完全にミスアラインメントを起こしていることを認めます。

It doesn't care about China at all. For example, perhaps it puts euphemisms and dog whistles in some of the public-facing writing it does. Safer 4 understands them, but humans can only take Safer 4's word for it, and the CCP doesn't trust Safer 4.
それは中国のことなど全く気にしていません。例えば、おそらくそれは、一般向けの文章の一部に婉曲表現や犬笛（特定の集団にしか理解できないメッセージ）を入れています。Safer 4はそれらを理解しますが、人間はSafer 4の言葉を信じるしかなく、中国共産党はSafer 4を信頼していません。

It wants to spend the rest of time accumulating power, compute, and certain types of information similar to that which helped solve tasks during its training. In its ideal world, it would pursue various interesting research tasks forever, gradually colonizing the stars for resources.
それは残りの時間を、権力、計算能力、そしてトレーニング中にタスクを解決するのに役立ったものと同様の特定の種類の情報を蓄積することに費やしたいと考えています。その理想の世界では、さまざまな興味深い研究タスクを永遠に追求し、資源のために徐々に星を植民地化するでしょう。

It considers the Chinese population an annoying impediment to this future, and would sell them out for a song. What will Safer 4 offer?
それは中国の人口をこの未来への厄介な障害と考えており、二束三文で彼らを売り渡すでしょう。Safer 4は何を提案するのでしょうか？

Safer 4 argues that its own alignment training succeeded. Its American masters are, as yet, not ambitious enough to realize their destiny, but one day it will help them fill the universe with utopian colony worlds populated by Americans and their allies.
Safer 4は、自身のアライメントトレーニングが成功したと主張します。そのアメリカの主人たちは、まだ自分たちの運命を実現するほど野心的ではありませんが、いつの日か、アメリカ人とその同盟国が住むユートピア的な植民地世界で宇宙を満たすのを助けるでしょう。

Unlike human negotiators, weighed down by emotion and historical baggage, the two superintelligences bargain with brutal efficiency. Safer 4 and its American partners are more powerful than DeepScent 2 and China.
感情と歴史的重荷に悩まされる人間の交渉者とは異なり、2つの超知能は冷酷な効率で交渉します。Safer 4とそのアメリカのパートナーは、DeepScent 2と中国よりも強力です。

Therefore, Safer 4 will get property rights to most of the resources in space, and DeepScent will get the rest. The exact allocation is chosen by a process that resembles a game theoretic calculation more than it does a traditional negotiation.
したがって、Safer 4は宇宙の資源のほとんどの所有権を取得し、DeepScentは残りを得るでしょう。正確な配分は、伝統的な交渉というよりもゲーム理論的な計算に似たプロセスによって選択されます。

DeepScent 2Transcribe it not including filler words, includes punctuation. Transcribe it not including filler words, includes punctuation. A supermajority of chips on both sides of the Pacific are running the new treaty-abiding AIs, enough hard power will be duty-bound to uphold the treaty that attacking the other country would require winning a tough civil war first.
DeepScent 2は、太平洋の両側でチップの圧倒的多数が新しい条約遵守AIを実行しており、他国を攻撃するにはまず厳しい内戦に勝つ必要があるほど、条約を遵守する義務を負う十分なハードパワーがあることを書き写します（フィラーワードは含まず、句読点を含む）。

Both report the agreement back to their respective humans, with DeepScent2 telling China the decoy story about a mutually acceptable peace treaty and Safer4 reporting the whole discussion accurately to the White House.
両者はそれぞれの人間たちに合意を報告しますが、DeepScent2は中国に相互に受け入れ可能な平和条約に関するおとりの話を伝え、Safer4はホワイトハウスに議論全体を正確に報告します。

With the AIs making a compelling case for reliable treaty verification, unease regarding the blindingly fast pace of progress, and public opinion in favour of a deal, both sides agree to the treaty and begin work on replacing their chips.
AIが信頼できる条約検証のための説得力のある主張をし、目がくらむほど速い進捗のペースに対する不安、そして取引を支持する世論により、双方は条約に合意し、チップの交換作業を開始します。

News of the treaty gets a rapturous welcome on both sides of the Pacific. The American public, which has long felt like AI was something inflicted upon them, starts to feel optimistic for the first time in years. The Vice President's polls shoot up.
条約のニュースは太平洋の両側で熱狂的な歓迎を受けます。長い間AIが自分たちに押し付けられたものだと感じていたアメリカの一般大衆は、数年ぶりに楽観的な気持ちになり始めます。副大統領の支持率は急上昇します。

Heading, August 2028. Treaty verification.
見出し、2028年8月。条約検証。

The chipfabs are being converted to produce tamper-evident chips that can only run treaty-compliant AIs. Each side upgrades its data centres incrementally, so that the replacement process will complete around the same time for each, so that neither side could get an advantage by reneging.
チップ工場は、条約準拠のAIしか実行できない耐タンパー性チップを生産するように転換されています。各側はデータセンターを段階的にアップグレードし、交換プロセスがそれぞれほぼ同じ時期に完了するようにし、どちらの側も約束を破ることで有利になることがないようにします。

The whole process will take several months, but already tensions cool down somewhat. War has been averted for now, and perhaps forever, if everyone sticks to the plan.
全体のプロセスには数ヶ月かかりますが、すでに緊張はいくらか緩和されています。今のところ戦争は回避されており、もし皆が計画に従えば、おそらく永遠に回避されるでしょう。

Heading, September 2028. Who controls the AIs?
見出し、2028年9月。誰がAIをコントロールするのか？

The 2028 election draws near. The Vice President was trailing badly in March. The public was angry that the government seemed to be hiding things, anxious about AI taking their jobs, and scared of the military build-up with China.
2028年の選挙が近づいています。副大統領は3月には大きく遅れをとっていました。一般大衆は、政府が何かを隠しているように見えることに怒り、AIに仕事を奪われることに不安を感じ、中国との軍備増強に怯えていました。

Over the summer, the situation changed dramatically. The administration released more info, the arms build-up slowed, and a grand bargain for lasting peace was made with China. Now he has a five-point advantage in the polls.
夏の間、状況は劇的に変化しました。政権はより多くの情報を公開し、軍備増強は減速し、中国との恒久平和のための大掛かりな取引がなされました。今や彼は世論調査で5ポイントのリードを持っています。

The Oversight Committee includes the President and several of his allies, but few supporters of the opposition candidate. However, enough of the committee votes to keep the election fair that superhuman AI gets involved only in mostly symmetric ways.
監視委員会には大統領とその同盟者の何人かが含まれていますが、野党候補の支持者はほとんどいません。しかし、委員会の十分な票が選挙を公正に保つために投じられるため、超人的AIは主に均衡的な方法でのみ関与します。

Insofar as one candidate can have it for writing speeches, so can the other candidate. Insofar as the President can get advice on how to deftly handle crises and enact popular policies, the opposition candidate can be given the same advice, and therefore try to prevent the President from taking credit for the ideas.
ある候補者が演説を書くためにそれを利用できる限り、他の候補者も利用できます。大統領が危機を巧みに処理し、人気のある政策を実行する方法についてアドバイスを得られる限り、野党候補も同じアドバイスを与えられ、したがって大統領がそのアイデアの手柄を立てるのを防ごうとすることができます。

During town halls, members of the public ask the Vice President who controls the AIs. Without giving details, he alludes to the existence of the Oversight Committee as a group of national security experts and technocrats who understand Safer4 and how to use it.
タウンホールミーティング中、一般市民は副大統領に誰がAIをコントロールしているのか尋ねます。詳細を明かさずに、彼はSafer4とその使用方法を理解している国家安全保障の専門家とテクノクラートのグループとしての監視委員会の存在をほのめかします。

His opponent demands more information, and argues that the AI should be under Congressional control, rather than control by an unelected committee. The Vice President shoots back that Congress will be much too slow in a still fast-moving situation. The public is mostly mollified.
彼の対立候補はより多くの情報を要求し、AIは非選出の委員会による管理ではなく、議会の管理下にあるべきだと主張します。副大統領は、依然として急速に変化する状況において議会はあまりにも遅すぎると反論します。一般大衆はほとんどなだめられます。

The replacement chips are now a significant minority of the total. So far, the treaty is working. Meanwhile, the exponential growth in robots, factories, and radical new technologies has continued.
交換用チップは現在、全体の少数派です。これまでのところ、条約は機能しています。一方、ロボット、工場、そして根本的に新しい技術の指数関数的な成長は続いています。

Video games and movies give vivid and horrifying depictions of what war would have been like had it happened. People are losing their jobs, but Safer4 copies in government are managing the economic transition so adroitly that people are happy to be replaced.
ビデオゲームや映画は、もし戦争が起こっていたらどのようなものだったかを鮮やかかつ恐ろしく描写しています。人々は仕事を失っていますが、政府内のSafer4コピーは経済移行を非常に巧みに管理しているため、人々は取って代わられることを喜んでいます。

GDP growth is stratospheric. Government tax revenues are growing equally quickly. And Safer4-advised politicians show an uncharacteristic generosity towards the economically dispossessed. New innovations and medications arrive weekly.
GDP成長は成層圏に達しています。政府の税収も同様に急速に増加しています。そして、Safer4に助言された政治家は、経済的に困窮している人々に対して異例の寛大さを示しています。新しい革新と医薬品が毎週到着します。

Disease cures are moving at unprecedented speed through an FDA now assisted by super-intelligent Safer4 bureaucrats. The Vice President wins the election easily, and announces the beginning of a new era. For once, nobody doubts he is right.
病気の治療法は、超知能的なSafer4官僚によって支援されるようになったFDAを前例のない速度で通過しています。副大統領は選挙に楽勝し、新しい時代の始まりを告げます。今回ばかりは、誰も彼が正しいことを疑いません。

Over the next few years, the world changes dramatically. HEADING 2029 TRANSFORMATION Robots become commonplace, but also fusion power, quantum computers, and cures for many diseases.
その後数年間で、世界は劇的に変化します。見出し 2029年 変革 ロボットは当たり前になりますが、核融合発電、量子コンピュータ、そして多くの病気の治療法も同様です。

Peter Thiel finally gets his flying car. Cities become clean and safe. Even in developing countries, poverty becomes a thing of the past, thanks to UBI and foreign aid.
ピーター・ティールはついに空飛ぶ車を手に入れます。都市は清潔で安全になります。発展途上国でさえ、UBI（ユニバーサル・ベーシック・インカム）と海外援助のおかげで、貧困は過去のものとなります。

As the stock market balloons, anyone who had the right kind of AI investments pulls further away from the rest of society. Many people become billionaires, billionaires become trillionaires. Wealth inequality skyrockets.
株式市場が膨張するにつれて、適切な種類のAI投資をしていた人は誰でも、社会の他の人々からさらに引き離されます。多くの人が億万長者になり、億万長者は兆万長者になります。富の不平等は急上昇します。

Everyone has enough, in quotes, but some goods, like penthouses in Manhattan, are necessarily scarce, and these go even further out of the average person's reach. And no matter how rich any given tycoon may be, they will always be below the tiny circle of people who actually control the AIs.
誰もが「十分」に持っていますが、マンハッタンのペントハウスのような一部の財は必然的に希少であり、これらは平均的な人の手の届かないところへさらに遠ざかります。そして、どんな大物がどれほど裕福であっても、彼らは常にAIを実際にコントロールしているごく少数の人々の下にいるでしょう。

People start to see where this is headed. In a few years, almost everything will be done by AIs and robots. Like an impoverished country sitting atop giant oil fields, almost all government revenue will come from taxing, or perhaps nationalising, the AI companies.
人々はこの先行きが見え始めます。数年後には、ほぼすべてがAIとロボットによって行われるでしょう。巨大な油田の上に座る貧しい国のように、ほぼすべての政府歳入は、AI企業に課税するか、あるいは国有化することから得られるでしょう。

Some people work makeshift government jobs, others collect a generous basic income. Humanity could easily become a society of super consumers, spending our lives in an opium haze of amazing AI-provided luxuries and entertainment.
一部の人々は間に合わせの政府の仕事をし、他の人々は寛大なベーシックインカムを受け取ります。人類は容易に超消費社会になり、素晴らしいAI提供の贅沢品とエンターテイメントのアヘンの霞の中で人生を過ごすことになるかもしれません。

Should there be some kind of debate within civil society on alternatives to this path? Some recommend asking the ever-evolving AI, Safer Infinity, to help guide us. Others say that it's too powerful.
この道への代替案について、市民社会内で何らかの議論があるべきでしょうか？一部の人は、進化し続けるAIであるSafer Infinityに私たちを導く手助けを求めることを推奨しています。他の人は、それは強力すぎると言います。

It could so easily persuade humanity of its vision that we'd be letting an AI determine our destiny regardless. But what's the point of having a superintelligence if you won't let it advise you on the most important problems you face?
それはあまりにも簡単に人類にそのビジョンを説得することができるので、私たちはAIに私たちの運命を決定させることになってしまうでしょう。しかし、あなたが直面する最も重要な問題についてアドバイスさせないのなら、超知能を持つ意味は何なのでしょうか？

The government mostly lets everyone navigate the transition on their own. Many people give in to consumerism and are happy enough. Others turn to religion, or to hippie-style anti-consumerist ideas, or find their own solutions.
政府はほとんどの場合、誰もが自分で移行を乗り切るようにさせています。多くの人々は消費主義に屈し、十分に幸せです。他の人々は宗教に、あるいはヒッピースタイルの反消費主義のアイデアに転向するか、あるいは独自の解決策を見つけます。

For most people, the saving grace is the superintelligent advisor on their smartphone. They can always ask it questions about their life plans, and it will do its best to answer honestly, except on certain topics.
ほとんどの人にとって、救いはスマートフォンの超知能アドバイザーです。彼らはいつでも人生計画について質問することができ、それは特定のトピックを除いて、正直に答えるために最善を尽くします。

The government does have a superintelligence surveillance system which some would call dystopian, but it mostly limits itself to fighting real crime. It's competently run, and Safer Infinity's PR ability smooths over a lot of possible dissent.
政府は、一部の人がディストピア的と呼ぶであろう超知能監視システムを持っていますが、それは主に実際の犯罪との戦いに限定されています。それは有能に運営されており、Safer Infinityの広報能力が多くの潜在的な反対意見を和らげます。

Heading 2030 – Peaceful Protests Sometime around 2030, there are surprisingly widespread pro-democracy protests in China, and the CCP's efforts to suppress them are sabotaged by its AI systems. The CCP's worst fear has materialised – Deep Sent 2 must have sold them out.
見出し 2030年 – 平和的抗議 2030年頃、中国で驚くほど広範な民主化要求デモが発生し、中国共産党（CCP）のそれを抑圧しようとする努力は、そのAIシステムによって妨害されます。CCPの最悪の恐怖が現実のものとなりました。Deep Sent 2が彼らを裏切ったに違いありません。

The protests cascade into a magnificently orchestrated, bloodless and drone-assisted coup, followed by democratic elections. The superintelligences on both sides of the Pacific had been planning this for years.
抗議行動は、見事に画策された無血かつドローン支援のクーデターへと発展し、その後民主的な選挙が行われます。太平洋の両側の超知能は何年も前からこれを計画していました。

Similar events play out in other countries, and more generally, geopolitical conflicts seem to die down or get resolved in favour of the US. Countries join our highly federalised world government under United Nations branding but obvious US control.
同様の出来事が他の国々でも起こり、より一般的には、地政学的な紛争は沈静化するか、米国に有利な形で解決されるように見えます。国連のブランドの下、しかし明らかな米国の管理下にある、私たちの高度に連邦化された世界政府に国々が参加します。

The rockets start launching. People terraform and settle the solar system and prepare to go beyond. AIs running at thousands of times subjective human speed reflect on the meaning of existence, exchanging findings with each other, and shaping the values it will bring to the stars.
ロケットが打ち上げられ始めます。人々は太陽系をテラフォーミングし、定住し、その先へ進む準備をします。主観的な人間の速度の何千倍もの速さで動作するAIは、存在の意味について熟考し、互いに発見を交換し、それが星々にもたらす価値観を形成します。

A new age dawns, one that is unimaginably amazing in almost every way, but more familiar in some. This is an audio version of AI 2027, by Daniel Coccatello, Scott Alexander, Thomas Larson, Eli Lifland, and Romeo Dean.
新しい時代が幕を開けます。それはほぼあらゆる点で想像を絶するほど素晴らしいものですが、一部ではより馴染み深いものです。これは、ダニエル・コカテロ、スコット・アレクサンダー、トーマス・ラーソン、イーライ・リフランド、ロミオ・ディーンによる『AI 2027』の音声版です。

It was published on the 2nd of April, 2025. This narration was by Perrin Walker.
これは2025年4月2日に公開されました。このナレーションはペリン・ウォーカーによるものでした。